{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e154e169",
   "metadata": {},
   "source": [
    "# Machine Learning Models for Text Classification\n",
    "\n",
    "In this notebook, we apply several machine learning models for text classification tasks. We preprocess the text data, train different models, and evaluate their performance using the classification report and accuracy score.\n",
    "\n",
    "## 1. Loading Data\n",
    "\n",
    "We begin by loading the preprocessed data (features and target variable) from saved pickle files.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load feature matrix and target variable\n",
    "X = pickle.load(open('Xtrain_matrix.pkl', 'rb'))\n",
    "y = pd.read_pickle('ytrain.pkl')\n",
    "```\n",
    "\n",
    "## 2. Data Splitting\n",
    "\n",
    "The data is split into training and testing sets using `train_test_split` from scikit-learn, where 80% of the data is used for training, and 20% for testing.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Logistic Regression\n",
    "\n",
    "Logistic Regression is applied to the training data. We use the multinomial option for multi-class classification and balance the class weights to handle imbalanced classes. The model is then evaluated on the test data.\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf_lr = linear_model.LogisticRegression(multi_class='multinomial', class_weight='balanced', max_iter=1000)\n",
    "clf_lr.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf_lr.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(f'Logistic Regression Accuracy: {clf_lr.score(X_test, y_test)}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Support Vector Machine (SVM)\n",
    "\n",
    "A Support Vector Machine (SVM) is trained with a polynomial kernel. The SVM is also evaluated using the classification report and accuracy score.\n",
    "\n",
    "```python\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf_svm = svm.SVC(gamma=0.01, kernel='poly')\n",
    "clf_svm.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf_svm.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(f'SVM Accuracy: {clf_svm.score(X_test, y_test)}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Random Forest Classifier\n",
    "\n",
    "Multiple configurations of the Random Forest Classifier are tested, including different values for the number of features (`max_features`) and the minimum number of samples required to split a node (`min_samples_split`). The model performance is evaluated using the classification report.\n",
    "\n",
    "```python\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# First configuration\n",
    "clf_rf = ensemble.RandomForestClassifier(n_jobs=-1, random_state=321)\n",
    "clf_rf.fit(X_train, y_train.values.ravel())\n",
    "y_pred = clf_rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Random Forest Accuracy: {clf_rf.score(X_test, y_test)}')\n",
    "\n",
    "# Second configuration\n",
    "clf_rf = ensemble.RandomForestClassifier(n_jobs=-1, max_features='sqrt', min_samples_split=4)\n",
    "clf_rf.fit(X_train, y_train.values.ravel())\n",
    "y_pred = clf_rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Random Forest Accuracy: {clf_rf.score(X_test, y_test)}')\n",
    "\n",
    "# Third configuration\n",
    "clf_rf = ensemble.RandomForestClassifier(n_jobs=-1, max_features='log2', min_samples_split=27)\n",
    "clf_rf.fit(X_train, y_train.values.ravel())\n",
    "y_pred = clf_rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Random Forest Accuracy: {clf_rf.score(X_test, y_test)}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. K-Nearest Neighbors (KNN) Classifier\n",
    "\n",
    "We train a K-Nearest Neighbors (KNN) classifier, first with a fixed value for the number of neighbors and later using GridSearchCV to find the best number of neighbors for the model. The model's performance is evaluated after each training.\n",
    "\n",
    "```python\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initial KNN model\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_train, y_train.values.ravel())\n",
    "y_pred = knn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'KNN Accuracy: {knn.score(X_test, y_test)}')\n",
    "\n",
    "# Grid search for optimal neighbors\n",
    "parametres = {'n_neighbors': np.arange(2, 28)}\n",
    "grid_knn = GridSearchCV(estimator=knn, param_grid=parametres)\n",
    "grid_knn.fit(X_train, y_train.values.ravel())\n",
    "print(f'Best parameters from GridSearchCV: {grid_knn.best_params_}')\n",
    "\n",
    "# Train KNN with optimal number of neighbors\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=27)\n",
    "knn.fit(X_train, y_train.values.ravel())\n",
    "y_pred = knn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'KNN Accuracy: {knn.score(X_test, y_test)}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Decision Tree Classifier\n",
    "\n",
    "We test two configurations of the Decision Tree Classifier, one with the `entropy` criterion and another with the `gini` criterion, to evaluate their performance on the text data.\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Decision Tree with entropy criterion\n",
    "dt_clf = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=123)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Decision Tree Accuracy (entropy): {dt_clf.score(X_test, y_test)}')\n",
    "\n",
    "# Decision Tree with gini criterion\n",
    "dt_clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=321)\n",
    "dt_clf_gini.fit(X_train, y_train)\n",
    "y_pred = dt_clf_gini.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Decision Tree Accuracy (gini): {dt_clf_gini.score(X_test, y_test)}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Voting Classifier\n",
    "\n",
    "We combine multiple classifiers (KNN, Random Forest, and Logistic Regression) into a Voting Classifier to improve the overall performance through ensemble learning.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define individual classifiers\n",
    "clf1 = KNeighborsClassifier(n_neighbors=27)\n",
    "clf2 = RandomForestClassifier(n_jobs=-1, max_features='sqrt', min_samples_split=4)\n",
    "clf3 = LogisticRegression(multi_class='multinomial', class_weight='balanced', max_iter=1000)\n",
    "\n",
    "# Create Voting Classifier\n",
    "vc = VotingClassifier(estimators=[('knn', clf1), ('rf', clf2), ('lr', clf3)], voting='hard')\n",
    "vc.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = vc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Voting Classifier Accuracy: {vc.score(X_test, y_test)}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. XGBoost Classifier\n",
    "\n",
    "Finally, we apply the XGBoost classifier to evaluate its performance on the text classification task.\n",
    "\n",
    "```python\n",
    "!pip install xgboost\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False)\n",
    "xgb_clf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'XGBoost Accuracy: {xgb_clf.score(X_test, y_test)}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Summary of Model Performance\n",
    "\n",
    "Each model was trained and evaluated based on the classification report and accuracy score. By comparing the results, we can determine which model performs best on the given text data. For future work, fine-tuning hyperparameters or trying additional models could further improve the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
