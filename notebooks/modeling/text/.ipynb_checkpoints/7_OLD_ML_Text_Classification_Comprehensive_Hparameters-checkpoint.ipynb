{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b2ffbb",
   "metadata": {},
   "source": [
    "# Machine Learning Models for Text Classification  \n",
    "\n",
    "In this notebook, we will train and evaluate different **Machine Learning models** for **text classification** using the **TF-IDF features** extracted in the previous steps.  \n",
    "\n",
    "We will use the **TF-IDF matrix (`Xtrain_matrix.pkl`)** as input features and the **target labels (`y_train_encoded.pkl`)** for supervised learning.  \n",
    "The **test dataset (`Xtest_matrix.pkl`)** will not be used at this stage, as it is reserved for **final result submission**.  \n",
    "\n",
    "We will start by experimenting with the following models:  \n",
    "- **Logistic Regression**  \n",
    "- **Support Vector Machines (SVM)**  \n",
    "- **Random Forest Classifier**  \n",
    "- **K-Neighbors Classifier**  \n",
    "- **Decision Tree Classifier**\n",
    "- **XGBoost**  \n",
    "- **Linear SVC** \n",
    "- **Voting Classifier**  \n",
    "\n",
    "ðŸ“Œ **These models have been selected based on standard text classification approaches, but we may adjust our choices depending on their performance.**  \n",
    "\n",
    "We will perform a **comprehensive hyperparameter search** for all models, optimizing for the **weighted F1-score**, as this metric is required to evaluate the classification performance in the context of the project and challenge.  \n",
    "\n",
    "By the end of this notebook, we will:  \n",
    "âœ” Compare the performance of different models  \n",
    "âœ” Optimize hyperparameters if necessary  \n",
    "âœ” Select and save the best-performing model for future use  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd1aba5",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c24a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model selection and hyperparameter tuning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c6179c",
   "metadata": {},
   "source": [
    "## 1. Loading Preprocessed Data  \n",
    "\n",
    "Before training our Machine Learning models, we need to load the **TF-IDF matrix (`Xtrain_matrix.pkl`)** and the **target labels (`ytrain.pkl`)**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46d7d448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root Directory: Data_Scientist_Rakuten_Project-main\n",
      "Using Config File from: config.py\n",
      "Loading TF-IDF matrix from: data\\processed\\text\\Xtrain_matrix.pkl\n",
      "Loading encoded labels from: data\\processed\\text\\y_train_encoded.pkl\n",
      "Data Successfully Loaded!\n",
      "X shape: (84916, 5000)\n",
      "y shape: (84916,)\n"
     ]
    }
   ],
   "source": [
    "# Get the current notebook directory\n",
    "CURRENT_DIR = Path(os.getcwd()).resolve()\n",
    "\n",
    "# Automatically find the project root (go up 3 levels)\n",
    "PROJECT_ROOT = CURRENT_DIR.parents[2]\n",
    "\n",
    "# Add project root to sys.path\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Function to get relative paths from project root\n",
    "def get_relative_path(absolute_path):\n",
    "    return str(Path(absolute_path).relative_to(PROJECT_ROOT))\n",
    "\n",
    "# Print project root directory\n",
    "print(f\"Project Root Directory: {PROJECT_ROOT.name}\")  # Display only the root folder name\n",
    "\n",
    "import config  # Now Python can find config.py\n",
    "\n",
    "# Paths to load\n",
    "tfidf_path = Path(config.XTRAIN_MATRIX_PATH)\n",
    "labels_path = Path(config.YTRAIN_ENCODED_PATH)\n",
    "\n",
    "# Print paths being used (relative to project root)\n",
    "print(f\"Using Config File from: {get_relative_path(config.__file__)}\")\n",
    "print(f\"Loading TF-IDF matrix from: {get_relative_path(tfidf_path)}\")\n",
    "print(f\"Loading encoded labels from: {get_relative_path(labels_path)}\")\n",
    "\n",
    "# Check if files exist before loading\n",
    "if not tfidf_path.exists():\n",
    "    raise FileNotFoundError(f\"Error: TF-IDF matrix file not found at {get_relative_path(tfidf_path)}\")\n",
    "\n",
    "if not labels_path.exists():\n",
    "    raise FileNotFoundError(f\"Error: Encoded labels file not found at {get_relative_path(labels_path)}\")\n",
    "\n",
    "# Load the TF-IDF matrix\n",
    "X = pickle.load(open(tfidf_path, \"rb\"))\n",
    "\n",
    "# Load the classification labels\n",
    "y = pd.read_pickle(labels_path)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Data Successfully Loaded!\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c541084c",
   "metadata": {},
   "source": [
    "## 2. Splitting Data into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e9cc783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Completed:\n",
      "X_train shape: (67932, 5000), y_train shape: (67932,)\n",
      "X_test shape: (16984, 5000), y_test shape: (16984,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and validation sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(f\"Split Completed:\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec869eb",
   "metadata": {},
   "source": [
    "## 3. Training Machine Learning Models  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbae9d2",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regression\n",
    "\n",
    "To optimize the **Logistic Regression** model, we perform a **GridSearchCV** on key hyperparameters, aiming to maximize the **weighted F1-score** to handle class imbalances effectively:\n",
    "\n",
    "- `multi_class`: Defines the type of classification (`multinomial` for multi-class problems).\n",
    "- `class_weight`: Balances classes for imbalanced data (`balanced`, `None`, or custom dictionaries).\n",
    "- `max_iter`: Controls the number of iterations for convergence.\n",
    "- `C`: Regularization strength, which can impact model performance.\n",
    "- `solver`: Specifies the algorithm for optimization (e.g., `lbfgs`, `saga`, `liblinear`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f482e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define parameter grid (reduced for efficiency)\n",
    "# param_grid = {\n",
    "#     'multi_class': ['multinomial'],\n",
    "#     'class_weight': ['balanced', None],\n",
    "#     'max_iter': [500, 1000, 1500]\n",
    "# }\n",
    "\n",
    "# # Initialize Logistic Regression model\n",
    "# log_reg = LogisticRegression()\n",
    "\n",
    "# # Perform GridSearchCV based on 'weighted F1-score'\n",
    "# grid_search = GridSearchCV(log_reg, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Extract best model\n",
    "# best_log_reg = grid_search.best_estimator_\n",
    "\n",
    "# # Display best hyperparameters\n",
    "# print(f\"Best Hyperparameters for Logistic Regression: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e807495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_lr = LogisticRegression(multi_class='multinomial', class_weight=None, max_iter=500)\n",
    "# clf_lr.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# # Predict and evaluate\n",
    "# y_pred = clf_lr.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73259a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_lr = LogisticRegression(multi_class='multinomial', class_weight='balanced', max_iter=1000)\n",
    "# clf_lr.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# # Predict and evaluate\n",
    "# y_pred = clf_lr.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a96fbb",
   "metadata": {},
   "source": [
    "- **GridSearchCV** for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a more extensive parameter grid\n",
    "param_grid = {\n",
    "    'multi_class': ['multinomial', 'ovr'],  # Essayer aussi 'ovr' pour comparer avec la classification binaire\n",
    "    'class_weight': ['balanced', None, 'dict'],  # Tester aussi un dictionnaire de poids\n",
    "    'max_iter': [500, 1000, 1500, 2000],  # Plus de valeurs pour voir l'impact\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Essayer diffÃ©rentes valeurs de rÃ©gularisation\n",
    "    'solver': ['lbfgs', 'saga', 'liblinear']  # Tester diffÃ©rents solveurs pour la rÃ©gression logistique\n",
    "}\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Perform GridSearchCV based on 'weighted F1-score'\n",
    "grid_search = GridSearchCV(log_reg, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract best model\n",
    "best_log_reg = grid_search.best_estimator_\n",
    "\n",
    "# Display best hyperparameters\n",
    "print(f\"Best Hyperparameters for Logistic Regression: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17d8f14",
   "metadata": {},
   "source": [
    "- **Re-train** Logistic Regression with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee82f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After GridSearchCV has finished\n",
    "# Retrain Logistic Regression with the best hyperparameters\n",
    "clf_lr = LogisticRegression(\n",
    "    multi_class=grid_search.best_params_['multi_class'],\n",
    "    class_weight=grid_search.best_params_['class_weight'],\n",
    "    max_iter=grid_search.best_params_['max_iter'],\n",
    "    C=grid_search.best_params_['C'],\n",
    "    solver=grid_search.best_params_['solver']\n",
    ")\n",
    "clf_lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3d2e0",
   "metadata": {},
   "source": [
    "- **Evaluate** Logistic Regression with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefe520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Logistic Regression model\n",
    "y_pred = clf_lr.predict(X_test)  # Make predictions on the test set\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for Logistic Regression:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optionally, calculate other evaluation metrics like F1-Score, Accuracy\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = clf_lr.score(X_test, y_test)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(f\"Weighted F1-Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cf0c39",
   "metadata": {},
   "source": [
    "- **Save** the best Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bcca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained Logistic Regression model to the appropriate path in the models/text directory\n",
    "with open(os.path.join(config.TEXT_MODELS_DIR, \"best_lr_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(best_log_reg, f)\n",
    "\n",
    "print(\"Best Logistic Regression model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e85a3",
   "metadata": {},
   "source": [
    "### 3.2 Support Vector Machines (SVM)\n",
    "\n",
    "To optimize the **Support Vector Machines (SVM)** model, we perform a **GridSearchCV** on key hyperparameters, aiming to maximize the **weighted F1-score** to effectively handle class imbalances:\n",
    "\n",
    "- `C`: Regularization parameter, controlling the trade-off between achieving a low error on the training set and minimizing the model complexity.\n",
    "- `kernel`: Specifies the kernel type to use in the algorithm (e.g., `linear`, `rbf`, `poly`, or `sigmoid`).\n",
    "- `class_weight`: Balances the classes for imbalanced data (`balanced`, `None`).\n",
    "- `max_iter`: Controls the maximum number of iterations for convergence.\n",
    "- `gamma`: Defines the kernel coefficient for `rbf`, `poly`, and `sigmoid` kernels. It can impact model flexibility and performance.\n",
    "- `degree`: Defines the degree of the polynomial kernel function (relevant only if `kernel='poly'`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ebe385",
   "metadata": {},
   "source": [
    "- **GridSearchCV** for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified parameter grid for SVM with focus on 'gamma', 'kernel' and 'C'\n",
    "param_grid = {\n",
    "    'C': [1, 10],  # Regularization parameter (simplified)\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # Focus on three kernel types\n",
    "    'gamma': [0.01, 0.1, 'scale'],  # Test simple gamma values\n",
    "}\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Perform GridSearchCV based on 'weighted F1-score'\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Display best hyperparametersprint(f\"Best Hyperparameters for SVM: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3830c79",
   "metadata": {},
   "source": [
    "- **Re-train** SVM with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece43eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After GridSearchCV has finished\n",
    "# Retrain SVM with the best hyperparameters\n",
    "best_svm = SVC(\n",
    "    C=grid_search.best_params_['C'],\n",
    "    kernel=grid_search.best_params_['kernel'],\n",
    "    gamma=grid_search.best_params_['gamma']\n",
    ")\n",
    "best_svm.fit(X_train, y_train)\n",
    "\n",
    "# If you want to evaluate the model (optional)\n",
    "# y_pred = best_svm.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Re-trained SVM model with the best hyperparameters: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97002d59",
   "metadata": {},
   "source": [
    "- **Evaluate** SVM with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d31215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate SVM model\n",
    "y_pred = best_svm.predict(X_test)  # Make predictions on the test set\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for SVM:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optionally, calculate other evaluation metrics like F1-Score, Accuracy\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = best_svm.score(X_test, y_test)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(f\"Weighted F1-Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b0316",
   "metadata": {},
   "source": [
    "- **Save** the best SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained SVM model to the appropriate path in the models/text directory\n",
    "with open(os.path.join(config.TEXT_MODELS_DIR, \"best_svm_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(best_svm, f)\n",
    "\n",
    "print(\"Best SVM model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41718c",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest Classifier\n",
    "\n",
    "To optimize the **Random Forest Classifier** model, we perform a **GridSearchCV** on key hyperparameters, aiming to maximize the **weighted F1-score** to effectively handle class imbalances:\n",
    "\n",
    "- `n_estimators`: The number of trees in the forest. More trees usually improve model performance, but also increase computation time.\n",
    "- `max_depth`: The maximum depth of the trees. Limiting the depth can prevent overfitting.\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node. This can control overfitting by setting higher values.\n",
    "- `min_samples_leaf`: The minimum number of samples required to be at a leaf node. A higher value can smooth the model.\n",
    "- `class_weight`: Balances the classes for imbalanced data (`balanced`, `None`).\n",
    "- `max_features`: The number of features to consider when looking for the best split. This can impact model performance and speed.\n",
    "- `bootstrap`: Whether bootstrap samples are used when building trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ffc74f",
   "metadata": {},
   "source": [
    "- **GridSearchCV** for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf830ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified parameter grid for Random\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],  # Number of trees in the forest (simplified range)\n",
    "    'min_samples_split': [2, 4, 5],  # Minimum samples required to split a node (simplified)\n",
    "    'max_features': ['auto', 'sqrt'],  # Number of features to consider at each split\n",
    "    'class_weight': ['balanced', None],  # Handle class imbalance\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Perform GridSearchCV based on 'weighted F1-score'\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Display best hyperparameters\n",
    "print(f\"Best Hyperparameters for Random Forest: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be95c0",
   "metadata": {},
   "source": [
    "- **Re-train** Random Forest with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49192d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After GridSearchCV has finished\n",
    "# Retrain Random Forest with the best hyperparameters\n",
    "best_rf = RandomForestClassifier(\n",
    "    n_estimators=grid_search.best_params_['n_estimators'],\n",
    "    min_samples_split=grid_search.best_params_['min_samples_split'],\n",
    "    max_features=grid_search.best_params_['max_features'],\n",
    "    class_weight=grid_search.best_params_['class_weight']\n",
    ")\n",
    "\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# If you want to evaluate the model (optional)\n",
    "# y_pred = best_rf.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Re-trained Random Forest model with the best hyperparameters: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10853ae",
   "metadata": {},
   "source": [
    "- **Evaluate** Random Forest with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest model\n",
    "y_pred = best_rf.predict(X_test)  # Make predictions on the test set\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for Random Forest:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optionally, calculate other evaluation metrics like F1-Score, Accuracy\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = best_rf.score(X_test, y_test)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(f\"Weighted F1-Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda9c81f",
   "metadata": {},
   "source": [
    "- **Save** the best Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187049ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained Random Forest model to the appropriate path in the models/text directory\n",
    "with open(os.path.join(config.TEXT_MODELS_DIR, \"best_rf_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(best_rf, f)\n",
    "\n",
    "print(\"Best Random Forest model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b49d5",
   "metadata": {},
   "source": [
    "### 3.4 K-Neighbors Classifier\n",
    "\n",
    "To optimize the **K-Neighbors Classifier** model, we perform a **GridSearchCV** on key hyperparameters, aiming to maximize the **weighted F1-score** to effectively handle class imbalances:\n",
    "\n",
    "- `n_neighbors`: The number of neighbors to use for knearest neighbors classification. Increasing this value makes the model more general and reduces overfitting.\n",
    "- `weights`: Function used to weight the points in the neighborhood. Options include `uniform` (all points are weighted equally) or `distance` (closer points have more influence).\n",
    "- `algorithm`: The algorithm used to compute the nearest neighbors (`auto`, `ball_tree`, `kd_tree`, `brute`).\n",
    "- `leaf_size`: The leaf size for the `ball_tree` and `kd_tree` algorithms. A smaller leaf size can improve search time.\n",
    "- `p`: The power parameter for the Minkowski distance. When `p=2`, this corresponds to the Euclidean distance.\n",
    "- `metric`: The distance metric to use. Can be `minkowski`, `manhattan`, `chebyshev`, or `cosine`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab04b18",
   "metadata": {},
   "source": [
    "- **GridSearchCV** for K-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8758e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified parameter grid for K-Neighbors\n",
    "param_grid = {\n",
    "    'n_neighbors': [2, 3, 5, 7, 27],  # Added n_neighbors = 2 and 27\n",
    "    'weights': ['uniform', 'distance'],  # Weight function for neighbors\n",
    "    'algorithm': ['auto', 'ball_tree'],  # Simplified algorithms to compute nearest neighbors\n",
    "}\n",
    "\n",
    "# Initialize the K-Neighbors Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform GridSearchCV based on 'weighted F1-score'\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract best model\n",
    "best_knn = grid_search.best_estimator_\n",
    "\n",
    "# Display best hyperparameters\n",
    "print(f\"Best Hyperparameters for K-Neighbors Classifier: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe5b666",
   "metadata": {},
   "source": [
    "- **Re-train** K-Neighbors Classifier with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65774e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After GridSearchCV has finished\n",
    "# Retrain K-Neighbors Classifier with the best hyperparameters\n",
    "best_knn = KNeighborsClassifier(\n",
    "    n_neighbors=grid_search.best_params_['n_neighbors'],\n",
    "    weights=grid_search.best_params_['weights'],\n",
    "    algorithm=grid_search.best_params_['algorithm']\n",
    ")\n",
    "\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# If you want to evaluate the model (optional)\n",
    "# y_pred = best_knn.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Re-trained K-Neighbors Classifier model with the best hyperparameters: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35be7f",
   "metadata": {},
   "source": [
    "- **Evaluate** K-Neighbors Classifier with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc53e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate K-Neighbors Classifier model\n",
    "y_pred = best_knn.predict(X_test)  # Make predictions on the test set\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for K-Neighbors Classifier:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optionally, calculate other evaluation metrics like F1-Score, Accuracy\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = best_knn.score(X_test, y_test)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(f\"Weighted F1-Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a36dc7",
   "metadata": {},
   "source": [
    "- **Save** the best K-Neighbors model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8afa3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained K-Neighbors model to the appropriate path in the models/text directory\n",
    "with open(os.path.join(config.TEXT_MODELS_DIR, \"best_knn_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(best_knn, f)\n",
    "\n",
    "print(\"Best K-Neighbors model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef24d6e",
   "metadata": {},
   "source": [
    "### 3.5 Decision Tree Classifier\n",
    "\n",
    "To optimize the **Decision Tree Classifier** model, we perform a **GridSearchCV** on key hyperparameters, aiming to maximize the **weighted F1-score** to effectively handle class imbalances:\n",
    "\n",
    "- `max_depth`: The maximum depth of the tree. Limiting the depth helps to prevent overfitting.\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node. This can control overfitting.\n",
    "- `min_samples_leaf`: The minimum number of samples required to be at a leaf node. A higher value can smooth the model.\n",
    "- `max_features`: The number of features to consider when looking for the best split. This can impact model performance and speed.\n",
    "- `criterion`: The function to measure the quality of a split. Common options include `gini` (Gini impurity) and `entropy` (information gain).\n",
    "- `class_weight`: Balances the classes for imbalanced data (`balanced`, `None`).\n",
    "- `splitter`: The strategy used to split at each node. Options include `best` (best split) or `random` (random split)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323553a",
   "metadata": {},
   "source": [
    "- **GridSearchCV** for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified parameter grid for Decision Tree Classifier with focus on 'max_features' and 'min_samples_split'\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10],  # Simplified range for maximum depth\n",
    "    'min_samples_split': [2, 5],  # Minimum samples required to split a node\n",
    "    'max_features': ['auto', 'sqrt'],  # Number of features to consider at each split\n",
    "    'class_weight': ['balanced', None],  # Handle class imbalance\n",
    "    'criterion': ['gini'],  # We can keep only 'gini' for simplicity\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Perform GridSearchCV based on 'weighted F1-score'\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract best model\n",
    "best_dt = grid_search.best_estimator_\n",
    "\n",
    "# Display best hyperparameters\n",
    "print(f\"Best Hyperparameters for Decision Tree: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7af8128",
   "metadata": {},
   "source": [
    "- **Re-train** Decision Tree with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After GridSearchCV has finished\n",
    "# Retrain Decision Tree with the best hyperparameters\n",
    "best_dt = DecisionTreeClassifier(\n",
    "    max_depth=grid_search.best_params_['max_depth'],\n",
    "    min_samples_split=grid_search.best_params_['min_samples_split'],\n",
    "    max_features=grid_search.best_params_['max_features'],\n",
    "    class_weight=grid_search.best_params_['class_weight'],\n",
    "    criterion=grid_search.best_params_['criterion']\n",
    ")\n",
    "\n",
    "best_dt.fit(X_train, y_train)\n",
    "\n",
    "# If you want to evaluate the model (optional)\n",
    "# y_pred = best_dt.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Re-trained Decision Tree model with the best hyperparameters: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225b272",
   "metadata": {},
   "source": [
    "- **Evaluate** Decision Tree with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb23db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Decision Tree model\n",
    "y_pred = best_dt.predict(X_test)  # Make predictions on the test set\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for Decision Tree:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optionally, calculate other evaluation metrics like F1-Score, Accuracy\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = best_dt.score(X_test, y_test)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(f\"Weighted F1-Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b14c9",
   "metadata": {},
   "source": [
    "- **Save** the best Decision Tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bd559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained Decision Tree model to the appropriate path in the models/text directory\n",
    "with open(os.path.join(config.TEXT_MODELS_DIR, \"best_dt_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(best_dt, f)\n",
    "\n",
    "print(\"Best Decision Tree model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94030bac",
   "metadata": {},
   "source": [
    "### 3.6 XGBoost\n",
    "\n",
    "To optimize the **XGBoost** model, we perform a **GridSearchCV** on key hyperparameters, aiming to maximize the **weighted F1-score** to effectively handle class imbalances:\n",
    "\n",
    "- `n_estimators`: The number of boosting rounds (trees) to build. More trees usually improve performance but increase computation time.\n",
    "- `learning_rate`: The rate at which the model learns. Smaller values make the model more robust but require more trees.\n",
    "- `max_depth`: The maximum depth of the decision trees. Increasing this can make the model more complex, potentially leading to overfitting.\n",
    "- `min_child_weight`: The minimum sum of instance weight (hessian) needed in a child. It can help control overfitting.\n",
    "- `subsample`: The fraction of samples used for fitting each tree. A lower value can help reduce overfitting.\n",
    "- `colsample_bytree`: The fraction of features to use for building each tree. It can help control overfitting.\n",
    "- `gamma`: The minimum loss reduction required to make a further partition. It helps control the complexity of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4eb2f3",
   "metadata": {},
   "source": [
    "- **GridSearchCV** for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf2402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified parameter grid for XGBoost with focus on the most important hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],  # Number of boosting rounds (simplified range)\n",
    "    'learning_rate': [0.05, 0.1],  # Learning rate (step size)\n",
    "    'max_depth': [3, 5],  # Maximum depth of the trees\n",
    "    'subsample': [0.8, 1.0],  # Fraction of samples to use for fitting each tree\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost model with use_label_encoder=False to suppress warnings\n",
    "xgb = XGBClassifier(use_label_encoder=False)\n",
    "\n",
    "# Perform GridSearchCV based on 'weighted F1-score'\n",
    "grid_search = GridSearchCV(xgb, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract best model\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Display best hyperparameters\n",
    "print(f\"Best Hyperparameters for XGBoost: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a964ee",
   "metadata": {},
   "source": [
    "- **Re-train** XGBoost with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967af31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After GridSearchCV has finished\n",
    "# Retrain XGBoost with the best hyperparameters\n",
    "best_xgb = XGBClassifier(\n",
    "    n_estimators=grid_search.best_params_['n_estimators'],\n",
    "    learning_rate=grid_search.best_params_['learning_rate'],\n",
    "    max_depth=grid_search.best_params_['max_depth'],\n",
    "    subsample=grid_search.best_params_['subsample'],\n",
    "    use_label_encoder=False  # Ensure to suppress the warning related to label encoding\n",
    ")\n",
    "\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# If you want to evaluate the model (optional)\n",
    "# y_pred = best_xgb.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Re-trained XGBoost model with the best hyperparameters: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996245cd",
   "metadata": {},
   "source": [
    "- **Evaluate** XGBoost with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate XGBoost model\n",
    "y_pred = best_xgb.predict(X_test)  # Make predictions on the test set\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for XGBoost:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optionally, calculate other evaluation metrics like F1-Score, Accuracy\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = best_xgb.score(X_test, y_test)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(f\"Weighted F1-Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624dd83",
   "metadata": {},
   "source": [
    "- **Save** the best XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e953f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained XGBoost model to the appropriate path in the models/text directory\n",
    "with open(os.path.join(config.TEXT_MODELS_DIR, \"best_xgb_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(best_xgb, f)\n",
    "\n",
    "print(\"Best XGBoost model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386176b",
   "metadata": {},
   "source": [
    "### 3.7 Linear SVC\n",
    "\n",
    "To optimize the **Linear SVC** model, we perform a **GridSearchCV** on key hyperparameters, aiming to maximize the **weighted F1-score** to effectively handle class imbalances:\n",
    "\n",
    "- `C`: Regularization parameter. It controls the trade-off between achieving a low error on the training data and minimizing the model complexity. A lower `C` encourages a simpler model.\n",
    "- `max_iter`: The maximum number of iterations for the solver to converge. Higher values allow the model to converge better, especially for complex data.\n",
    "- `class_weight`: Balances the classes for imbalanced data (`balanced`, `None`).\n",
    "- `penalty`: Specifies the norm used in the penalization. Can be `'l1'` or `'l2'`. `l2` is the default and commonly used for Linear SVC.\n",
    "- `dual`: A boolean flag to choose between the primal or dual formulation. Typically, use `dual=True` for sparse data and `dual=False` for dense data.\n",
    "- `tol`: The tolerance for stopping criteria. Lower values result in higher precision but longer computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c2401",
   "metadata": {},
   "source": [
    "- **GridSearchCV** for Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified parameter grid for Linear SVC with focus on 'C', 'max_iter', and 'tol'\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Regularization parameter (simplified)\n",
    "    'max_iter': [2000, 3000],  # Maximum number of iterations for solver to converge (simplified range)\n",
    "    'class_weight': ['balanced', None],  # Handle class imbalance\n",
    "    'penalty': ['l2'],  # Penalty type (only 'l2' as specified)\n",
    "    'dual': [False],  # Use dual=False for dense data (default and recommended for LinearSVC)\n",
    "    'tol': [1e-4, 1e-5]  # Tolerance for stopping criteria (simplified range)\n",
    "}\n",
    "\n",
    "# Initialize the Linear SVC model\n",
    "linear_svc = LinearSVC()\n",
    "\n",
    "# Perform GridSearchCV based on 'weighted F1-score'\n",
    "grid_search = GridSearchCV(linear_svc, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract best model\n",
    "best_lsvc = grid_search.best_estimator_\n",
    "\n",
    "# Display best hyperparameters\n",
    "print(f\"Best Hyperparameters for Linear SVC: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36cc85",
   "metadata": {},
   "source": [
    "- **Re-train** Linear SVC with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef7671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After GridSearchCV has finished\n",
    "# Retrain Linear SVC with the best hyperparameters\n",
    "best_lsvc = LinearSVC(\n",
    "    C=grid_search.best_params_['C'],\n",
    "    max_iter=grid_search.best_params_['max_iter'],\n",
    "    class_weight=grid_search.best_params_['class_weight'],\n",
    "    penalty=grid_search.best_params_['penalty'],\n",
    "    dual=grid_search.best_params_['dual'],\n",
    "    tol=grid_search.best_params_['tol']\n",
    ")\n",
    "\n",
    "best_lsvc.fit(X_train, y_train)\n",
    "\n",
    "# If you want to evaluate the model (optional)\n",
    "# y_pred = best_lsvc.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Re-trained Linear SVC model with the best hyperparameters: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dce114",
   "metadata": {},
   "source": [
    "- **Evaluate** Linear SVC with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbe756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Linear SVC model\n",
    "y_pred = best_lsvc.predict(X_test)  # Make predictions on the test set\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for Linear SVC:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optionally, calculate other evaluation metrics like F1-Score, Accuracy\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = best_lsvc.score(X_test, y_test)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(f\"Weighted F1-Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9867a75c",
   "metadata": {},
   "source": [
    "- **Save** the best Linear SVC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained Linear SVC model to the appropriate path in the models/text directory\n",
    "with open(os.path.join(config.TEXT_MODELS_DIR, \"best_lsvc_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(best_lsvc, f)\n",
    "\n",
    "print(\"Best Linear SVC model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b2a51",
   "metadata": {},
   "source": [
    "## 4. Creating a Voting Classifier\n",
    "- **Combine** all the best models trained in the previous steps using a **Voting Classifier**.\n",
    "- Create a **hard voting** or **soft voting** strategy depending on the models' compatibility with `predict_proba()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c43adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the best models from the previous steps\n",
    "models = [\n",
    "    ('lr', best_lr),  # Logistic Regression\n",
    "    ('svm', best_svm),  # Support Vector Machine\n",
    "    ('rf', best_rf),  # Random Forest\n",
    "    ('knn', best_knn),  # K-Neighbors Classifier\n",
    "    ('dt', best_dt),  # Decision Tree\n",
    "    ('xgb', best_xgb),  # XGBoost\n",
    "    ('lsvc', best_lsvc)  # Linear SVC\n",
    "]\n",
    "\n",
    "# Check if any model supports predict_proba(), which is required for soft voting\n",
    "if any([hasattr(model[1], 'predict_proba') for model in models]):\n",
    "    voting_clf = VotingClassifier(estimators=models, voting='soft')  # Soft voting\n",
    "else:\n",
    "    voting_clf = VotingClassifier(estimators=models, voting='hard')  # Hard voting\n",
    "\n",
    "# Train the Voting Classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Optionally, save the Voting Classifier\n",
    "with open(os.path.join(config.MODELS_DIR, \"voting_clf_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(voting_clf, f)\n",
    "\n",
    "print(f\"Voting Classifier trained with {'soft' if voting_clf.voting == 'soft' else 'hard'} voting and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f092309",
   "metadata": {},
   "source": [
    "- **Evaluate** Voting Classifier with the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Voting Classifier model\n",
    "y_pred = voting_clf.predict(X_test)  # Make predictions on the test set\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for Voting Classifier:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optionally, calculate other evaluation metrics like F1-Score, Accuracy\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = voting_clf.score(X_test, y_test)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(f\"Weighted F1-Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473296a",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Selection\n",
    "- **Compare** the performance of each individual model and the **Voting Classifier** using metrics like the **weighted F1-score**.\n",
    "- Select the best-performing model for further use or future deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Create a dictionary to store the models and their respective predictions\n",
    "models = {\n",
    "    'Logistic Regression': best_lr,\n",
    "    'SVM': best_svm,\n",
    "    'Random Forest': best_rf,\n",
    "    'K-Neighbors': best_knn,\n",
    "    'Decision Tree': best_dt,\n",
    "    'XGBoost': best_xgb,\n",
    "    'Linear SVC': best_lsvc,\n",
    "    'Voting Classifier': voting_clf  # The Voting Classifier\n",
    "}\n",
    "\n",
    "# Initialize an empty dictionary to store the weighted F1 scores\n",
    "f1_scores = {}\n",
    "\n",
    "# Compare the performance of each model on the test set\n",
    "for model_name, model in models.items():\n",
    "    # Make predictions using the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the weighted F1-score for the model\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1_scores[model_name] = f1\n",
    "\n",
    "    # Print the classification report for each model\n",
    "    print(f\"Classification Report for {model_name}:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Select the best-performing model based on the weighted F1-score\n",
    "best_model_name = max(f1_scores, key=f1_scores.get)\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nBest model based on weighted F1-score: {best_model_name} with a score of {f1_scores[best_model_name]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91b5ad",
   "metadata": {},
   "source": [
    "### Display the Mapping Between Encoded Labels and Original Classes\n",
    "Before saving the best model, let's display the correspondence between the **encoded labels** (0-26) and their **original classes**. This will help us understand the mapping of the product categories in the context of our model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e34f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.PRDTYPECODE_MAPPING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b094ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the label mapping from the pickle file (Encoded Label â†’ Original Class)\n",
    "with open(config.PRDTYPECODE_MAPPING_PATH, 'rb') as f:\n",
    "    prdtypecode_mapping = pickle.load(f)\n",
    "\n",
    "# The DataFrame has already the three columns: \"Original prdtypecode\", \"Encoded target\", and \"Label\"\n",
    "mapping_df = prdtypecode_mapping  # Since it's already a DataFrame\n",
    "\n",
    "# Display the first 5 rows of the DataFrame after loading\n",
    "print(\"Label mapping loaded from pickle file:\")\n",
    "print(mapping_df.head())\n",
    "\n",
    "# Predict using the best model (selected based on weighted F1-score)\n",
    "y_pred_best_model = best_model.predict(X_test)  # Make predictions with the best model\n",
    "\n",
    "# Create a DataFrame to display the classification report and results for each class\n",
    "classification_results = classification_report(y_test, y_pred_best_model, output_dict=True)\n",
    "classification_results_df = pd.DataFrame(classification_results).transpose()\n",
    "\n",
    "# Merge the classification results with the mapping\n",
    "classification_results_with_mapping = pd.merge(classification_results_df, mapping_df, \n",
    "                                               left_index=True, right_on=\"Encoded target\", how=\"left\")\n",
    "\n",
    "# Display the table showing classification results and the corresponding classes\n",
    "print(\"Classification Report with Encoded Labels and Their Original Classes:\\n\")\n",
    "print(classification_results_with_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66d91ba",
   "metadata": {},
   "source": [
    "## 6. Saving the Best Model \n",
    "- **Save** the best model (Voting Classifier or other) for future use and potential deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model (either Voting Classifier or another model)\n",
    "best_model_path = os.path.join(config.MODELS_DIR, \"best_model.pkl\")\n",
    "\n",
    "with open(best_model_path, \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"The best model has been saved at: {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e99123",
   "metadata": {},
   "source": [
    "## 7. ðŸ”„ Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b68089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
