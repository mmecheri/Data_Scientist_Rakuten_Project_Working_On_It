{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4df90b",
   "metadata": {},
   "source": [
    "#  Convolutional Neural Networks (CNNs) Model Name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Objective:**  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Steps:**  \n",
    "1. **Import Required Libraries and Setup**: Load the necessary libraries and set up project paths and configurations.  \n",
    "2. **Import Preprocessed Data**: Load the tokenized and padded text data from the previous notebook.  \n",
    "3. **Model Construction**: Define an initial **CNN Model** as a baseline model.  \n",
    "4. **Training the Model**: Train the model using the prepared dataset.  \n",
    "5. **Evaluation**: Perform a **customized performance analysis** to assess the model's effectiveness on test data.  \n",
    "6. **Hyperparameter Tuning**: Given computational constraints, we focus on optimizing only the most relevant parameters.  \n",
    "7. **Save Models**: Save both the **CNN Model\"** and the **best optimized model** for future use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab4475",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Deep Learning (TensorFlow/Keras)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, backend as K\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, RNN, GRUCell, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Machine Learning & Model Evaluation\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline  \n",
    "\n",
    "import importlib\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd564672",
   "metadata": {},
   "source": [
    "### Setting Up Project Paths and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd6e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current notebook directory\n",
    "CURRENT_DIR = Path(os.getcwd()).resolve()\n",
    "\n",
    "# Automatically find the project root (go up 2 level)\n",
    "PROJECT_ROOT = CURRENT_DIR.parents[2]\n",
    "\n",
    "# Add project root to sys.path\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Function to get relative paths from project root\n",
    "def get_relative_path(absolute_path):\n",
    "    return str(Path(absolute_path).relative_to(PROJECT_ROOT))\n",
    "\n",
    "# Print project root directory\n",
    "print(f\"Project Root Directory: {PROJECT_ROOT.name}\")  # Display only the root folder name\n",
    "\n",
    "import config  # Now Python can find config.py\n",
    "importlib.reload(config)  # Reload config to ensure any updates are applied\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40cdcd4",
   "metadata": {},
   "source": [
    "## 2. Import Preprocessed Data\n",
    "\n",
    "In this step, we will import the preprocessed data that has already been tokenized and padded, with the original training dataset split into training and testing sets using `train_test_split`. The data we are importing includes:\n",
    "\n",
    "\n",
    "- **X_train_split_tokenized**: The training data that has been tokenized, padded, and split from the original dataset.\n",
    "- **X_test_split_tokenized**: The testing data that has been tokenized, padded, and split from the original dataset.\n",
    "- **y_train_split**: The corresponding target labels for the training data, also split after applying `train_test_split`.\n",
    "- **y_test_split**: The corresponding target labels for the testing data, after splitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for datasets\n",
    "xtrain_tokenized_path = Path(config.X_TRAIN_SPLIT_TOKENIZED_PATH)\n",
    "xtest_tokenized_path = Path(config.X_TEST_SPLIT_TOKENIZED_PATH)\n",
    "y_train_split_path = Path(config.Y_TRAIN_SPLIT_PATH)\n",
    "y_test_split_path = Path(config.Y_TEST_SPLIT_PATH)\n",
    "\n",
    "\n",
    "#  Load tokenizer\n",
    "tokenizer_path = Path(config.TOKENIZER_PATH)\n",
    "                    \n",
    "# Function to get relative paths from project root\n",
    "def get_relative_path(absolute_path: Path):\n",
    "    \"\"\"Returns the relative path from the project root.\"\"\"\n",
    "    return str(absolute_path.relative_to(config.BASE_DIR))\n",
    "\n",
    "# Function to load a Pickle file safely\n",
    "def load_pickle(file_path: Path, dataset_name: str):\n",
    "    \"\"\"Loads a pickle file with error handling and basic visualization.\"\"\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"Error: `{dataset_name}` file not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        data = pd.read_pickle(file_path)\n",
    "        print(f\"Successfully loaded `{dataset_name}` | Shape: {data.shape}\")\n",
    "        \n",
    "#         # Display first rows if dataset is not empty\n",
    "#         if not data.empty:\n",
    "#             display(data.head())\n",
    "\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading `{dataset_name}`: {e}\")\n",
    "        return None\n",
    "\n",
    "        return None, None\n",
    "\n",
    "def load_tokenizer(tokenizer_path: Path):\n",
    "    \"\"\"Loads the saved tokenizer and determines the correct vocabulary size.\"\"\"\n",
    "    if not tokenizer_path.exists():\n",
    "        print(f\"Error: `Tokenizer` not found at {tokenizer_path}\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        with open(tokenizer_path, 'rb') as handle:\n",
    "            tokenizer = pickle.load(handle)\n",
    "\n",
    "        # Check if num_words was set during training\n",
    "        if tokenizer.num_words is not None:\n",
    "            vocab_size = tokenizer.num_words  # Use the predefined vocabulary limit\n",
    "        else:\n",
    "            vocab_size = len(tokenizer.word_index) + 1  # Otherwise, use the full vocabulary\n",
    "\n",
    "        # Print vocabulary details separately\n",
    "        print(f\"Tokenizer successfully loaded.\")\n",
    "        print(f\"Vocabulary size used (vocab_size): {vocab_size}\")\n",
    "        print(f\"Total words found in dataset: {len(tokenizer.word_index)}\")\n",
    "\n",
    "        return tokenizer, vocab_size\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "    \n",
    "# List of required files with their names\n",
    "required_files = {\n",
    "    \"Tokenized Training Dataset\": xtrain_tokenized_path,\n",
    "    \"Tokenized Testing Dataset\": xtest_tokenized_path,\n",
    "    \"Training Labels\": y_train_split_path,\n",
    "    \"Testing Labels\": y_test_split_path,\n",
    "    \"Tokenizer\": tokenizer_path  \n",
    "}\n",
    "\n",
    "# Check if files exist before loading\n",
    "for name, path in required_files.items():\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Error: `{name}` file not found at {get_relative_path(path)}\")\n",
    "\n",
    "# Load datasets\n",
    "X_train = load_pickle(xtrain_tokenized_path, \"X_train_TokenizationSequencing.pkl\")\n",
    "X_test = load_pickle(xtest_tokenized_path, \"X_test_TokenizationSequencing.pkl\") # Submission dataset\n",
    "y_train = load_pickle(y_train_split_path, \"y_train_split.pkl\")\n",
    "y_test = load_pickle(y_test_split_path, \"y_test_split.pkl\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer, vocab_size = load_tokenizer(tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e745370c",
   "metadata": {},
   "source": [
    "## 3. CNN Model Construction\n",
    "\n",
    "In this step, we construct an RNN model using a **CNN Model** layer for images classification. The architecture includes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a02699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14aa9604",
   "metadata": {},
   "source": [
    "## 4. Model Compilation, Training, and Evaluation\n",
    "\n",
    "Before fine-tuning, we start with a **initial version** of our **GRU-based RNN model** for multi-class text classification. This helps establish a **baseline** while **keeping training time and computational cost manageable**.  \n",
    "\n",
    "#### **Model Parameters and Rationale**  \n",
    "\n",
    "**Number of Epochs**: `EPOCHS = 5`  \n",
    "Training for **5 epochs** is a good starting point because **text classification tasks** often require multiple epochs to capture meaningful patterns. We can increase the number of epochs later if needed.  \n",
    "\n",
    "**Batch Size**: `BATCH_SIZE = 200`  \n",
    "- A batch size of `200` balances **memory usage and convergence speed**.  \n",
    "- Since **text sequences** can be long, using a **moderate batch size** prevents excessive **GPU/CPU memory consumption**.  \n",
    "**Loss Function**: `sparse_categorical_crossentropy`  \n",
    "- Best suited for **multi-class classification** with integer-labeled categories.  \n",
    "\n",
    "**Optimizer**: `Adam (learning_rate=0.001)`  \n",
    "- **Adam** (Adaptive Moment Estimation) is used because **it adjusts the learning rate automatically and helps the model learn faster and more efficiently**.\n",
    "- We set `learning_rate=0.001`, which is the **default value**, but this can be fine-tuned later.  \n",
    "\n",
    "**Metrics**: `accuracy` (excluding `weighted_f1` from `metrics`)  \n",
    "- `accuracy` is a **quick and interpretable metric** for an initial evaluation.  \n",
    "- If the **custom `weighted_f1` function does not work correctly**, we will compute the **Weighted F1-score manually** after evaluation to ensure accurate results.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755882ee",
   "metadata": {},
   "source": [
    "### 4.1 Custom Function for Weighted F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596fe058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Custom function to calculate weighted F1 score==> TO BE TEST\n",
    "# def weighted_f1(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Computes the weighted F1-score for multi-class classification.\n",
    "\n",
    "#     TensorFlow tensors are converted to NumPy arrays since sklearn's f1_score \n",
    "#     requires NumPy input. However, TensorFlow functions do not support native NumPy \n",
    "#     operations, so we use `tf.py_function` to wrap it.\n",
    "\n",
    "#     Args:\n",
    "#         y_true (tf.Tensor): True labels.\n",
    "#         y_pred (tf.Tensor): Predicted labels.\n",
    "\n",
    "#     Returns:\n",
    "#         tf.Tensor: Weighted F1-score computed using sklearn.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Convert one-hot encoded predictions to class indices\n",
    "#     y_true = tf.argmax(y_true, axis=-1)\n",
    "#     y_pred = tf.argmax(y_pred, axis=-1)\n",
    "\n",
    "#     def compute_f1(y_true_np, y_pred_np):\n",
    "#         \"\"\"Helper function to compute F1-score in NumPy space.\"\"\"\n",
    "#         return f1_score(y_true_np, y_pred_np, average='weighted')\n",
    "\n",
    "#     # Use tf.py_function to make the sklearn function compatible with TensorFlow\n",
    "#     return tf.py_function(func=compute_f1, inp=[y_true, y_pred], Tout=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45121466",
   "metadata": {},
   "source": [
    "### 4.2 Model Compilation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# # Define Early Stopping callback\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_loss',    # Monitor validation loss\n",
    "#     patience=3,            # Stop training if no improvement after 3 epochs\n",
    "#     restore_best_weights=True  # Restore model weights from the best epoch\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # Compile the model \n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Set learning rate explicitly\n",
    "#     loss='sparse_categorical_crossentropy',  # For integer-labeled data\n",
    "#      metrics=['accuracy'] ## Only accuracy is included; weighted F1 will be computed manually\n",
    "# #    metrics=['accuracy', weighted_f1]  # Include both accuracy and weighted F1-score : ISSUE HERE\n",
    "# )\n",
    "\n",
    "# # Define training parameters as variables\n",
    "# BATCH_SIZE = 200\n",
    "# EPOCHS = 50\n",
    "\n",
    "# # Train the model using the defined variables\n",
    "# history = model.fit(          \n",
    "#     X_train, \n",
    "#     y_train.values, \n",
    "#     batch_size=BATCH_SIZE, \n",
    "#     epochs=EPOCHS, \n",
    "#     validation_data=(X_test, y_test.values),\n",
    "#      callbacks=[early_stopping] \n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458191a3",
   "metadata": {},
   "source": [
    "### 4.3 Saving the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(config)  # Reload config to ensure any updates are applied\n",
    "# from datetime import datetime  # Import datetime for timestamp\n",
    "\n",
    "# # Define model name prefix (Change this for different architectures)\n",
    "# MODEL_NAME = \"gru_model\"  # Example: \"gru_model\", \"lstm_model\", etc.\n",
    "\n",
    "# # Ensure the directory exists before saving the model\n",
    "# os.makedirs(config.NEURAL_MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# # Get current date and time in YYYYMMDD-HHMM format\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "# # Retrieve the number of epochs completed\n",
    "# if 'history' in globals():\n",
    "#     num_epochs = len(history.epoch)  # Actual trained epochs\n",
    "# else:\n",
    "#     num_epochs = \"unknown\"\n",
    "\n",
    "# # Define a compact filename with model name, epochs, and timestamp\n",
    "# model_filename = f\"{MODEL_NAME}_E{EPOCHS}-T{num_epochs}_{timestamp}.h5\"\n",
    "\n",
    "# # Full path for saving the model\n",
    "# model_path = os.path.join(config.NEURAL_MODELS_DIR, model_filename)\n",
    "\n",
    "# # Print the path where the model will be saved\n",
    "# print(f\"Saving the model to: {model_path}\")\n",
    "\n",
    "# try:\n",
    "#     # Save the trained Keras model (in .h5 format)\n",
    "#     model.save(model_path)\n",
    "\n",
    "#     print(f\"Model saved successfully as {model_filename}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     # Print error message if saving fails\n",
    "#     print(f\"Error occurred while saving the model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04c23a",
   "metadata": {},
   "source": [
    "### 4.4 Load the saved Trained Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac96fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(config)  #Reload config to apply any updates\n",
    "\n",
    "# # Ask the user for the model name to load\n",
    "# model_name = \"gru_model_E50-T8_20250225-0907.h5\"  # Change this to the desired model filename\n",
    "\n",
    "# # Define the full path to the model\n",
    "# model_path = os.path.join(config.NEURAL_MODELS_DIR, model_name)\n",
    "\n",
    "# # Check if the model file exists\n",
    "# if os.path.exists(model_path):\n",
    "#     # Load the trained Keras model\n",
    "#     model = tf.keras.models.load_model(model_path)\n",
    "#     print(f\"✔ Model loaded successfully from: {model_path}\")\n",
    "# else:\n",
    "#     print(f\"[X] Model file not found at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73fa2d6",
   "metadata": {},
   "source": [
    "### 4.5 Model Evaluation on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc259265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Evaluate the model on the test data\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test.values)\n",
    "\n",
    "# # Print evaluation results\n",
    "# print(f\"[✔] Test Loss: {test_loss:.4f}\")\n",
    "# print(f\"[✔] Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# # Predict class probabilities on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Convert probabilities to class labels\n",
    "# y_pred_classes = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "# # Compute weighted F1-score manually\n",
    "# test_f1 = f1_score(y_test, y_pred_classes, average='weighted', zero_division=0)\n",
    "# print(f\"[✔] Test Weighted F1-Score (computed manually): {test_f1:.4f}\")\n",
    "\n",
    "# # Print classification report\n",
    "# print(\"\\n[✔] Classification Report:\")\n",
    "# print(classification_report(y_test, y_pred_classes,zero_division=0))\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# conf_matrix_df_raw = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# # Plot the confusion matrix\n",
    "# plt.figure(figsize=(18, 12))\n",
    "# sns.heatmap(conf_matrix_df_raw, annot=True, fmt='g', cmap='Blues', xticklabels=np.arange(27), yticklabels=np.arange(27))\n",
    "\n",
    "# # Add titles and labels\n",
    "# plt.title('Confusion Matrix for GRUCell Model', fontsize=16)\n",
    "# plt.xlabel('Predicted Labels', fontsize=12)\n",
    "# plt.ylabel('True Labels', fontsize=12)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad088fd",
   "metadata": {},
   "source": [
    "## 6. Customized Model Performance Analysis\n",
    "\n",
    "This section outlines our comprehensive evaluation of the model’s classification performance, structured as follows:\n",
    "\n",
    "- **6.1 Customized Classification Report:**  \n",
    "  We start by mapping the training class numbers to the original class numbers and labels, providing clarity on performance metrics across the original categories.\n",
    "\n",
    "- **6.2 Classification Report Generation & Class Categorization:**  \n",
    "  Here, we categorize classes based on their F1-scores using thresholds derived from the Rakuten benchmark (weighted F1-score = 0.8113). This approach helps us distinguish classes that are well-classified, moderately classified, or poorly classified, thereby pinpointing areas for targeted improvement.\n",
    "\n",
    "- **6.3 Consolidated Misclassification Report:**  \n",
    "  We then dive deeper into prediction errors with a multi-layered analysis that includes:  \n",
    "  - **6.3.1 Confusion Matrix Visualization with Mapped Labels:** Visualizing misclassification patterns by aligning training class numbers with the original labels.  \n",
    "  - **6.3.2 Breakdown of Misclassifications:** Identifying common misclassification trends and frequently confused classes.  \n",
    "  - **6.3.3 Detailed Misclassification Statistics:** Providing error counts and misclassification rates to highlight areas where the model underperforms.\n",
    "\n",
    "This structured approach allows us to quickly identify strengths and focus on specific areas for improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b3cd8",
   "metadata": {},
   "source": [
    "###  6.1 Customized Classification Report with Mapped Classes and Original Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac4cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import src.model_evaluation  # Initial import\n",
    "# importlib.reload(config)  # Force reload the module\n",
    "# importlib.reload(src.model_evaluation)  # Force reload the module\n",
    "\n",
    "# from src.model_evaluation import process_classification_report\n",
    "# # import config\n",
    "\n",
    "# # Generate and process the classification report\n",
    "# classification_df, summary_rows, accuracy_value, accuracy_support = process_classification_report(\n",
    "#     y_test, y_pred_classes, config.PRDTYPECODE_MAPPING_PATH\n",
    "# )\n",
    "\n",
    "# # Display accuracy separately\n",
    "# print(f\"\\n[✔] Accuracy: {accuracy_value:.2f} (on {accuracy_support} samples)\\n\")\n",
    "\n",
    "# # Display final classification summary\n",
    "# print(\"\\n[✔] Classification Summary Rows\")\n",
    "# display(summary_rows)\n",
    "\n",
    "# # Display processed classification report\n",
    "# print(\"\\n[✔] Processed Classification Report with Mapped Classes and Original Labels\")\n",
    "# display(classification_df.tail(40))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4174d048",
   "metadata": {},
   "source": [
    "### 6.2 Customized Classification Report Generation and  Class Categorization\n",
    "\n",
    "To assess performance, classes are categorized based on their **F1-scores** using thresholds defined relative to the **Rakuten benchmark** (RNN on text data, weighted F1-score = **0.8113**). These thresholds are designed to pinpoint areas for improvement and align with industry standards:\n",
    "\n",
    "- **Well-classified:** **F1 ≥ Benchmark (e.g., 0.80)** – Categories that perform on par with or better than the reference model.\n",
    "- **Moderately classified:** **Threshold_lower ≤ F1 < Benchmark** – Categories that may require further refinement.\n",
    "- **Poorly classified:** **F1 < Threshold_lower** – Categories with notable misclassification issues.\n",
    "\n",
    "These thresholds are **customizable** and will be adjusted based on results from baseline models in this challenge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacdb840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import src.model_evaluation  # Initial import\n",
    "# import src.display # Initial import\n",
    "\n",
    "# importlib.reload(config)  # Force reload the module\n",
    "# importlib.reload(src.model_evaluation)  # Force reload the module\n",
    "# importlib.reload(src.display)  # Force reload the module\n",
    "# from src.model_evaluation import analyze_classification_performance, process_classification_report\n",
    "# from src.display import display_dataframe_to_user  # Import de la fonction\n",
    "\n",
    "# # Generate and process the classification report\n",
    "# classification_df, summary_rows, accuracy_value, accuracy_support = process_classification_report(\n",
    "#     y_test, y_pred_classes, config.PRDTYPECODE_MAPPING_PATH\n",
    "# )\n",
    "\n",
    "\n",
    "# # Define classification thresholds\n",
    "# WELL_CLASSIFIED_THRESHOLD = 0.80\n",
    "# MODERATELY_CLASSIFIED_THRESHOLD = 0.50\n",
    "\n",
    "# # Run classification performance analysis with dynamic thresholds\n",
    "# category_counts, well_classified, moderately_classified, poorly_classified = analyze_classification_performance(\n",
    "#     classification_df, \n",
    "#     well_classified_threshold=WELL_CLASSIFIED_THRESHOLD, \n",
    "#     moderately_classified_threshold=MODERATELY_CLASSIFIED_THRESHOLD\n",
    "# )\n",
    "\n",
    "# # Display summary based on dynamic thresholds\n",
    "# print(\"\\n[✔] Summary of Classification Performance:\")\n",
    "# print(f\"- {category_counts.get('Well-classified', 0)} classes have an F1-score ≥ {WELL_CLASSIFIED_THRESHOLD:.2f} (Well-classified)\")\n",
    "# print(f\"- {category_counts.get('Moderate', 0)} classes have an F1-score between {MODERATELY_CLASSIFIED_THRESHOLD:.2f} and {WELL_CLASSIFIED_THRESHOLD:.2f} (Moderately classified)\")\n",
    "# print(f\"- {category_counts.get('Poorly classified', 0)} classes have an F1-score < {MODERATELY_CLASSIFIED_THRESHOLD:.2f} (Poorly classified)\")\n",
    "\n",
    "\n",
    "# # Display detailed classification breakdown\n",
    "# # If `display_rows` is None (default), the entire DataFrame is shown; otherwise, it limits to `display_rows`.\n",
    "# display_dataframe_to_user(\n",
    "#     name=f\"Well-Classified Categories (F1 ≥ {WELL_CLASSIFIED_THRESHOLD:.2f})\",\n",
    "#     dataframe=well_classified.sort_values(by=\"f1-score\", ascending=False), display_rows=None\n",
    "# )\n",
    "\n",
    "# display_dataframe_to_user(\n",
    "#     name=f\"Moderately Classified Categories ({MODERATELY_CLASSIFIED_THRESHOLD:.2f} ≤ F1 < {WELL_CLASSIFIED_THRESHOLD:.2f})\",\n",
    "#     dataframe=moderately_classified.sort_values(by=\"f1-score\", ascending=False), display_rows=None\n",
    "# )\n",
    "\n",
    "# display_dataframe_to_user(\n",
    "#     name=f\"Poorly Classified Categories (F1 < {MODERATELY_CLASSIFIED_THRESHOLD:.2f})\",\n",
    "#     dataframe=poorly_classified.sort_values(by=\"f1-score\", ascending=True),display_rows=None\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39314706",
   "metadata": {},
   "source": [
    " ### 6.3 Consolidated Misclassification Report: Analyzing Prediction Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59c439",
   "metadata": {},
   "source": [
    "### 6.3.1 Confusion Matrix Visualization with Mapped Labels\n",
    "→ Display the confusion matrix with mapped labels to visualize misclassification patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cca761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(config)  # Reload config to ensure any updates are applied\n",
    "# import src.model_evaluation  # Initial import\n",
    "\n",
    "# importlib.reload(src.model_evaluation)  # Force reload the module\n",
    "# from src.model_evaluation import generate_confusion_matrix  \n",
    "\n",
    "# # Generate the confusion matrix with mapped labels\n",
    "# conf_matrix_df_labeled = generate_confusion_matrix(y_test, y_pred_classes, config.PRDTYPECODE_MAPPING_PATH)\n",
    "\n",
    "# # Display the confusion matrix as a heatmap\n",
    "# plt.figure(figsize=(20, 12))\n",
    "# sns.heatmap(conf_matrix_df_labeled, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=0.5)\n",
    "# plt.xlabel(\"Predicted Labels\")\n",
    "# plt.ylabel(\"True Labels\")\n",
    "# plt.title(\"Confusion Matrix with Mapped Labels\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67f3ca",
   "metadata": {},
   "source": [
    "### 6.3.2 Breakdown of Misclassifications  \n",
    "→ **Analyze misclassification trends, identify the most frequently misclassified classes, and assess overall model performance.**  \n",
    "\n",
    "In this section, we analyze the model’s misclassification patterns using a structured approach.  \n",
    "We present four breakdowns to understand where the model struggles the most.  \n",
    "\n",
    "#### **Columns in the Misclassification Table:**  \n",
    "- **Encoded target** → The encoded numerical representation of each class.  \n",
    "- **Original prdtypecode** → The original product category code.  \n",
    "- **Class Label** → The human-readable label of the class.  \n",
    "- **Samples** → The total number of instances for each class.  \n",
    "- **Misclassified Count** → The number of misclassified samples for the class.  \n",
    "- **Misclassification Rate (%)** → The percentage of misclassified samples within the class.  \n",
    "- **Global Misclassification Rate (%)** → The percentage of misclassified samples relative to the total dataset.  \n",
    "\n",
    "####  **Key Misclassification Perspectives**  \n",
    "\n",
    "1. **Classes with the highest number of samples**  \n",
    "   - Shows the most **frequent classes** in the dataset.  \n",
    "   - Helps assess whether a class’s misclassification is due to **its high occurrence** or **its inherent difficulty**.  \n",
    "   - Large classes can dominate misclassification counts even if their misclassification rate is low.  \n",
    "\n",
    "2. **Classes with the highest number of misclassifications**  \n",
    "   - Highlights the classes with the **largest absolute number of errors**.  \n",
    "   - Useful for identifying which classes **contribute the most errors**, regardless of their misclassification rate.  \n",
    "   - A class with many misclassifications may indicate **model weakness** or **data imbalance**.  \n",
    "\n",
    "3. **Classes with the highest misclassification rate (%)**  \n",
    "   - Identifies the classes where the model **struggles the most** proportionally.  \n",
    "   - A high rate suggests **systematic confusion** in distinguishing this class from others.  \n",
    "   - Small classes with a **high error rate** might require **better feature representation** or **more training data**.  \n",
    "\n",
    "4. **Classes contributing the most to overall misclassification (%)**  \n",
    "   - Measures **each class’s impact on total model errors**.  \n",
    "   - A class with **many samples and a moderate error rate** can still contribute significantly to overall misclassification.  \n",
    "   - Helps prioritize which classes should be improved to **reduce total model error** the most.  \n",
    "\n",
    "\n",
    "By examining these four aspects, we gain insights into both **systematic classification errors** and **dataset imbalances** that could be influencing the model’s performance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e31e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "\n",
    "# importlib.reload(config)\n",
    "# import src.model_evaluation  # Initial import\n",
    "# importlib.reload(src.model_evaluation)\n",
    "# from src.model_evaluation import analyze_and_display_misclassification\n",
    "\n",
    "# # Calculs déjà réalisés\n",
    "# misclassified_counts = conf_matrix_df_labeled.sum(axis=1) - np.diag(conf_matrix_df_labeled)\n",
    "# total_samples_per_class = conf_matrix_df_labeled.sum(axis=1)\n",
    "# misclassification_rates = round((misclassified_counts / total_samples_per_class) * 100, 2)\n",
    "\n",
    "# # [✔]Call the function with a custom number of rows to display\n",
    "# # You can set `display_rows` to control how many rows are shown in the notebook.\n",
    "# # By default, it displays 10 rows.\n",
    "# misclassification_analysis_df = analyze_and_display_misclassification(\n",
    "#     y_test, y_pred_classes, misclassified_counts, total_samples_per_class, misclassification_rates, display_rows=10\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc714a4",
   "metadata": {},
   "source": [
    "### 6.3.3 Detailed Misclassification Statistics  \n",
    "→ **Analyze individual misclassification cases, including total errors, misclassification rates, and overprediction trends.**  \n",
    "\n",
    "####  Meaning of Columns in the Classification Error Analysis Table  \n",
    "\n",
    "| **Column**                | **Description** |\n",
    "|---------------------------|----------------|\n",
    "| **True Label**            | The actual class of the sample in the test set (the correct category). |\n",
    "| **Predicted Label**       | The incorrect class predicted by the model instead of the **True Label**. |\n",
    "| **Count**                 | The number of times samples from **True Label** were misclassified as **Predicted Label**. |\n",
    "| **Percentage**            | The proportion of this specific misclassification relative to **all misclassified instances**. Computed as: `(Count / Total Misclassified) * 100`. |\n",
    "| **Class Sample Size** | The total number of test samples belonging to the **True Label** class (i.e., how many instances of this class exist in the test set). |\n",
    "| **Misclassified Count**   | The total number of **True Label** samples that were misclassified (sum of all errors for this class). |\n",
    "| **Misclassification Rate** | The error rate for the **True Label** class. Computed as: `(Misclassified Count /Class Sample Size) * 100`. |\n",
    "| **Total Predicted**       | The total number of test samples predicted as belonging to the **Predicted Label** class (both correctly and incorrectly). |\n",
    "| **Incorrect Predictions** | The number of test samples **incorrectly predicted** as **Predicted Label** (should have been a different class). |\n",
    "| **Overprediction Rate**   | The proportion of incorrect predictions for the **Predicted Label** class. Computed as: `(Incorrect Predictions / Total Predicted) * 100`. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba650bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# importlib.reload(config) \n",
    "# import src.model_evaluation  # Initial import\n",
    "# importlib.reload(src.model_evaluation)  # Force reload to use updated functions\n",
    "# from src.model_evaluation import generate_display_misclass_report\n",
    "\n",
    "# # Generate the enriched misclassification report\n",
    "# # NOTE: To display only a subset of rows, modify display_rows when calling the function\n",
    "# # Example: Use display_rows=20 to show only the top 20 rows:\n",
    "# misclassification_report = generate_display_misclass_report(\n",
    "#     y_test, y_pred_classes, config.PRDTYPECODE_MAPPING_PATH, display_rows=20 # Set top_n to an integer (e.g., 20) to limit rows\n",
    "# )\n",
    "\n",
    "# # # Display the final enriched report\n",
    "# # print(\"\\n[✔] Consolidated Misclassification Report:\")\n",
    "# # display(misclassification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a26da3",
   "metadata": {},
   "source": [
    "### 6.4 **Exporting Classification Analysis to Excel** \n",
    "\n",
    "This step saves all classification insights into a **single Excel file** with multiple sheets for easy review.  \n",
    "\n",
    "####  Exported Data Includes:\n",
    "- **Well-Classified, Moderately Classified, Poorly Classified** categories (sorted by F1-score).  \n",
    "- **Misclassification Analysis** (error rates, misclassified counts).  \n",
    "- **Consolidated Misclassification Report** (detailed misclassification breakdown per class).  \n",
    "- **Raw Confusion Matrix** (numerical values without labels).  \n",
    "\n",
    "####  Output File Example:  \n",
    "**`classification_analysis_<MODEL_NAME>_<TIMESTAMP>.xlsx`**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db785437",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Generate and process the classification report\n",
    "# classification_df, summary_rows, accuracy_value, accuracy_support = process_classification_report(\n",
    "#     y_test, y_pred_classes, config.PRDTYPECODE_MAPPING_PATH\n",
    "# )\n",
    "# print(type(classification_df))\n",
    "# print(type(summary_rows))\n",
    "# print(type(accuracy_value))\n",
    "# print(type(accuracy_support))\n",
    "    \n",
    "# # Display processed classification report\n",
    "# print(\"\\n[✔] Processed Classification Report with Mapped Classes and Original Labels\")\n",
    "# display(classification_df.tail(5))\n",
    "\n",
    "# # Display accuracy separately\n",
    "# print(f\"\\n[✔] Accuracy: {accuracy_value:.2f} (on {accuracy_support} samples)\\n\")\n",
    "\n",
    "# # Display final classification summary\n",
    "# print(\"\\n[✔] Classification Summary Rows\")\n",
    "# display(summary_rows.tail(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b46b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import src.model_evaluation  # Initial import\n",
    "# import src.export_utils  # # Initial import\n",
    "\n",
    "# importlib.reload(config) # Force reload module\n",
    "# importlib.reload(src.model_evaluation)  # Force reload module\n",
    "# importlib.reload(src.export_utils)  # Force reload module\n",
    "\n",
    "# from src.model_evaluation import analyze_and_display_misclassification, generate_display_misclass_report\n",
    "# from src.export_utils import export_all_analysis\n",
    "\n",
    "\n",
    "# # Define model name and file prefix\n",
    "# model_used = \"GRU_Model\"\n",
    "# file_prefix = \"classification_analysis\"\n",
    "\n",
    "# # Generate and process the classification report\n",
    "# classification_df, summary_rows, accuracy_value, accuracy_support = process_classification_report(\n",
    "#     y_test, y_pred_classes, config.PRDTYPECODE_MAPPING_PATH\n",
    "# )\n",
    "\n",
    "\n",
    "# # Convert conf_matrix_df_raw to Pandas DataFrame\n",
    "# conf_matrix_df = pd.DataFrame(conf_matrix_df_raw, \n",
    "#                               index=[f\"True Class {label}\" for label in set(y_test)],  # Index as True Labels\n",
    "#                               columns=[f\"Pred Class {label}\" for label in set(y_pred_classes)])  # Columns as Predicted Labels\n",
    "\n",
    "\n",
    "\n",
    "# # Generates and displays misclassification analysis\n",
    "# full_misclassification_analysis_df = analyze_and_display_misclassification(y_test,\n",
    "#                                                                            y_pred_classes,\n",
    "#                                                                            misclassified_counts,\n",
    "#                                                                            total_samples_per_class,\n",
    "#                                                                            misclassification_rates,\n",
    "#                                                                            display_rows=None,\n",
    "#                                                                            display_output=False\n",
    "# )\n",
    "\n",
    "# #Generates and displays a comprehensive misclassification report combining multiple analyses\n",
    "# full_misclassification_report = generate_display_misclass_report(y_test,\n",
    "#                                                                  y_pred_classes,\n",
    "#                                                                  config.PRDTYPECODE_MAPPING_PATH,\n",
    "#                                                                  display_rows=None,\n",
    "#                                                                 display_output=False\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# dataframes_to_export = {\n",
    "#     \"Class Categorization\": {\n",
    "#         f\"Well-Classified (F1 ≥ {WELL_CLASSIFIED_THRESHOLD:.2f})\": well_classified.sort_values(by=\"f1-score\", ascending=False),\n",
    "#         f\"Moderately Classified ({MODERATELY_CLASSIFIED_THRESHOLD:.2f} ≤ F1 < {WELL_CLASSIFIED_THRESHOLD:.2f})\": moderately_classified.sort_values(by=\"f1-score\", ascending=False),\n",
    "#         f\"Poorly Classified (F1 < {MODERATELY_CLASSIFIED_THRESHOLD:.2f})\": poorly_classified.sort_values(by=\"f1-score\", ascending=True),\n",
    "#     },\n",
    "#     \"Misclassification Analysis\": full_misclassification_analysis_df,\n",
    "#     \"Consolidated Misclassification Report\": full_misclassification_report,\n",
    "#     \"Raw Confusion Matrix\": conf_matrix_df\n",
    "# }\n",
    "\n",
    "# # Export all the data\n",
    "# export_path = export_all_analysis(\n",
    "#     dataframes_dict=dataframes_to_export,\n",
    "#     model_name=model_used,\n",
    "#     file_prefix=file_prefix,\n",
    "#     mode=\"text\",\n",
    "#     classification_df=classification_df,\n",
    "#     summary_rows=summary_rows,\n",
    "#     accuracy_value=accuracy_value,\n",
    "#     accuracy_support=accuracy_support\n",
    "# )\n",
    "\n",
    "# # Print confirmation\n",
    "# print(f\"\\n[✔] Full classification analysis saved to: {export_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757bb746",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning (Optional – Depending on Time & Resources) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f674e",
   "metadata": {},
   "source": [
    "### 7.1 Hyperparameter Search for GRU Cell Model using RandomizedSearchCV  \n",
    "\n",
    "In this section, we perform **hyperparameter tuning** for the GRU-based text classification model using **RandomizedSearchCV**. Given computational constraints, we focus on optimizing only the **dropout rate** and **number of epochs**, while keeping other parameters fixed. This approach ensures an efficient search while maintaining model performance.  \n",
    "\n",
    "#### **Fixed Hyperparameters:**  \n",
    "- **Batch Size = 200**: Initial results showed stable performance with `batch_size=200`, making it a reliable choice.  \n",
    "\n",
    "#### **Limited Hyperparameter Search:**  \n",
    "- **Dropout Rate (0.2, 0.3)**: Helps control overfitting and improve generalization.  \n",
    "- **Epochs (5, 8)**: Ensures training is not too short (underfitting) or too long (wasting resources).  \n",
    "\n",
    "#### **Efficient Computation:**  \n",
    "- **n_jobs=1**: Prevents system overload and excessive parallelization.  \n",
    "- **cv=2**: Balances validation stability and computational cost.  \n",
    "- **n_iter=4**: Covers all possible combinations of `dropout_rate` and `epochs`.  \n",
    "- **Early Stopping**: Automatically stops training if validation accuracy doesn’t improve, optimizing resource usage.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ed7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Function to create the model\n",
    "# def create_model(dropout_rate=0.2, epochs=5):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(10000, 200))  # Embedding layer\n",
    "#     model.add(RNN(GRUCell(128), return_sequences=True))  # GRU Layer (Fixed at 128 units)\n",
    "#     model.add(Dropout(dropout_rate))  # Dropout layer\n",
    "#     model.add(GlobalAveragePooling1D())  # Global Average Pooling\n",
    "#     model.add(Dense(256, activation='relu'))  # Dense Layer\n",
    "#     model.add(Dropout(dropout_rate))  # Dropout layer\n",
    "#     model.add(Dense(27, activation='softmax'))  # Output Layer\n",
    "\n",
    "#     # Use Adam optimizer with fixed learning rate\n",
    "#     optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "#     model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881b73e",
   "metadata": {},
   "source": [
    "### 7.2 Retraining the GRUCell Model with Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3570327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from scikeras.wrappers import KerasClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Fixed batch size\n",
    "# BATCH_SIZE = 200\n",
    "\n",
    "# # Wrap the Keras model with Sci-Keras KerasClassifier\n",
    "# model = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# # Define early stopping (stop training if val_accuracy doesn’t improve for 3 epochs)\n",
    "# early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "# # Hyperparameter grid to search (only dropout rate and epochs)\n",
    "# param_dist = {\n",
    "#     'model__dropout_rate': [0.2, 0.3],  # Pass as model__dropout_rate for KerasClassifier\n",
    "#     'model__epochs': [5, 8],  # Pass as model__epochs\n",
    "# }\n",
    "\n",
    "# # Use RandomizedSearchCV for hyperparameter tuning\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=model, \n",
    "#     param_distributions=param_dist, \n",
    "#     n_iter=4,  # Cover all possible combinations\n",
    "#     cv=2,  # Reduce cross-validation folds for efficiency\n",
    "#     verbose=2, \n",
    "#     n_jobs=-1  \n",
    "# )\n",
    "\n",
    "# # Fit the RandomizedSearchCV on the training data\n",
    "# random_search.fit(X_train, y_train.values)\n",
    "\n",
    "# # Print the best hyperparameters found\n",
    "# print(f\"Best Hyperparameters: {random_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae065c5",
   "metadata": {},
   "source": [
    " ### 7.3 Evaluating the Performance of the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==========================================================================================\n",
    "# # Evaluate the Best Model Performance\n",
    "# # ==========================================================================================\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred_best = best_model.predict(X_test)\n",
    "# y_pred_best = y_pred_best.argmax(axis=1)  # Convert one-hot or probability outputs to class labels if necessary\n",
    "\n",
    "# # Compute evaluation metrics\n",
    "# accuracy = accuracy_score(y_test, y_pred_best)\n",
    "# weighted_f1 = f1_score(y_test, y_pred_best, average=\"weighted\")\n",
    "\n",
    "# # Print classification report\n",
    "# print(f\"[✔] Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"[✔] Weighted F1-Score: {weighted_f1:.4f}\\n\")\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred_best))\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "# # Plot the confusion matrix\n",
    "# plt.figure(figsize=(18, 12))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(27), yticklabels=range(27))\n",
    "# plt.xlabel(\"Predicted Label\")\n",
    "# plt.ylabel(\"True Label\")\n",
    "# plt.title(\"Confusion Matrix for the Best GRU Cell Model\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d308b1",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30412f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
