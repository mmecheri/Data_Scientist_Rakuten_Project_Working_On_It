{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65532141",
   "metadata": {},
   "source": [
    "# DL Text Tokenization and Sequencing  \n",
    "\n",
    "To effectively train **Deep Learning models** for text classification, raw textual data must be transformed into a structured numerical format that neural networks can process. Unlike **traditional Machine Learning models** that leverage **TF-IDF** or **Bag-of-Words (BoW)** for feature extraction, deep learning architectures rely on **embedding-based representations** to capture semantic relationships between words.  \n",
    "\n",
    "In this notebook, we focus on **preparing text data for deep learning models** by performing the following key steps:  \n",
    "\n",
    "- **Tokenization**: Converting text into a vocabulary of indexed tokens using Keras' `Tokenizer`.  \n",
    "- **Sequence Encoding**: Mapping words to their corresponding integer representations.  \n",
    "- **Padding Sequences**: Ensuring uniform input size by applying zero-padding to sequences.  \n",
    "\n",
    "These transformations are crucial for feeding textual data into **Recurrent Neural Networks (RNNs)**, **Long Short-Term Memory Networks (LSTMs)**, **Gated Recurrent Units (GRUs)**, **1D Convolutional Neural Networks (Conv1D)**, and **Deep Neural Networks (DNNs)**. Each of these architectures expects fixed-length input sequences and benefits from structured token representations.  \n",
    "\n",
    "By the end of this notebook, we will have a **tokenized and padded dataset**, ready for deep learning model training in the next phase.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27762c84",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4add82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa3876",
   "metadata": {},
   "source": [
    "### Setting Up Project Paths and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20388ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root Directory: Data_Scientist_Rakuten_Project-main\n"
     ]
    }
   ],
   "source": [
    "# Get the current notebook directory\n",
    "CURRENT_DIR = Path(os.getcwd()).resolve()\n",
    "\n",
    "# Automatically find the project root (go up 1 level)\n",
    "PROJECT_ROOT = CURRENT_DIR.parents[1]\n",
    "\n",
    "# Add project root to sys.path\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Function to get relative paths from project root\n",
    "def get_relative_path(absolute_path):\n",
    "    return str(Path(absolute_path).relative_to(PROJECT_ROOT))\n",
    "\n",
    "# Print project root directory\n",
    "print(f\"Project Root Directory: {PROJECT_ROOT.name}\")  # Display only the root folder name\n",
    "\n",
    "import config  # Now Python can find config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66b7c2",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data\n",
    "\n",
    "Before applying tokenization and sequencing, we will load the final cleaned datasets (`X_train_final.pkl` and `X_test_final.pkl`).  \n",
    "Additionally, we will load the encoded target labels (`y_train_encoded.pkl`) that were previously prepared during the TF-IDF vectorization step.  \n",
    "Reusing these labels ensures consistency during the training of our deep learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c077015b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded `X_train_final_encoded.pkl` | Shape: (84916, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>prdtypecode_encoded</th>\n",
       "      <th>Label</th>\n",
       "      <th>image_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Adult Books</td>\n",
       "      <td>image_1263597046_product_3804725264.jpg</td>\n",
       "      <td>olivia personalisiertes notizbuch seiten punkt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Des Arts (Le) N¬∞ 133 Du 28/09/2001 - L...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>2280</td>\n",
       "      <td>1</td>\n",
       "      <td>Magazines</td>\n",
       "      <td>image_1008141237_product_436067568.jpg</td>\n",
       "      <td>journal arts art marche salon art asiatique pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>Video Games Accessories</td>\n",
       "      <td>image_938777978_product_201115110.jpg</td>\n",
       "      <td>grand stylet ergonomique bleu gamepad nintendo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>1280</td>\n",
       "      <td>3</td>\n",
       "      <td>Toys for Children</td>\n",
       "      <td>image_457047496_product_50418756.jpg</td>\n",
       "      <td>peluche donald europe disneyland marionnette d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>2705</td>\n",
       "      <td>4</td>\n",
       "      <td>Books</td>\n",
       "      <td>image_1077757786_product_278535884.jpg</td>\n",
       "      <td>guerre tuques luc idees grandeur veut organise...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) N¬∞ 133 Du 28/09/2001 - L...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4                               La Guerre Des Tuques   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                               <NA>  3804725264  1263597046   \n",
       "1                                               <NA>   436067568  1008141237   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978   \n",
       "3                                               <NA>    50418756   457047496   \n",
       "4  Luc a des id&eacute;es de grandeur. Il veut or...   278535884  1077757786   \n",
       "\n",
       "   prdtypecode  prdtypecode_encoded                    Label  \\\n",
       "0           10                    0              Adult Books   \n",
       "1         2280                    1                Magazines   \n",
       "2           50                    2  Video Games Accessories   \n",
       "3         1280                    3        Toys for Children   \n",
       "4         2705                    4                    Books   \n",
       "\n",
       "                                image_name  \\\n",
       "0  image_1263597046_product_3804725264.jpg   \n",
       "1   image_1008141237_product_436067568.jpg   \n",
       "2    image_938777978_product_201115110.jpg   \n",
       "3     image_457047496_product_50418756.jpg   \n",
       "4   image_1077757786_product_278535884.jpg   \n",
       "\n",
       "                                                text  \n",
       "0  olivia personalisiertes notizbuch seiten punkt...  \n",
       "1  journal arts art marche salon art asiatique pa...  \n",
       "2  grand stylet ergonomique bleu gamepad nintendo...  \n",
       "3  peluche donald europe disneyland marionnette d...  \n",
       "4  guerre tuques luc idees grandeur veut organise...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded `y_train_encoded.pkl` | Shape: (84916,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    2\n",
       "3    3\n",
       "4    4\n",
       "Name: prdtypecode, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded `X_test_Submission_final.pkl` | Shape: (13812, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>image_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84916</th>\n",
       "      <td>Folkmanis Puppets - 2732 - Marionnette Et Th√©√¢...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>516376098</td>\n",
       "      <td>1019294171</td>\n",
       "      <td>image_1019294171_product_516376098.jpg</td>\n",
       "      <td>folkmanis puppets marionnette theatre mini turtle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84917</th>\n",
       "      <td>Porte Flamme Gaxix - Flamebringer Gaxix - 136/...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>133389013</td>\n",
       "      <td>1274228667</td>\n",
       "      <td>image_1274228667_product_133389013.jpg</td>\n",
       "      <td>porte flamme gaxix flamebringer twilight dragons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84918</th>\n",
       "      <td>Pompe de filtration Speck Badu 95</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4128438366</td>\n",
       "      <td>1295960357</td>\n",
       "      <td>image_1295960357_product_4128438366.jpg</td>\n",
       "      <td>pompe filtration speck badu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84919</th>\n",
       "      <td>Robot de piscine √©lectrique</td>\n",
       "      <td>&lt;p&gt;Ce robot de piscine d&amp;#39;un design innovan...</td>\n",
       "      <td>3929899732</td>\n",
       "      <td>1265224052</td>\n",
       "      <td>image_1265224052_product_3929899732.jpg</td>\n",
       "      <td>robot piscine electrique robot design innovant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84920</th>\n",
       "      <td>Hsm Destructeur Securio C16 Coupe Crois¬øE: 4 X...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>152993898</td>\n",
       "      <td>940543690</td>\n",
       "      <td>image_940543690_product_152993898.jpg</td>\n",
       "      <td>hsm destructeur securio coupe croise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             designation  \\\n",
       "84916  Folkmanis Puppets - 2732 - Marionnette Et Th√©√¢...   \n",
       "84917  Porte Flamme Gaxix - Flamebringer Gaxix - 136/...   \n",
       "84918                  Pompe de filtration Speck Badu 95   \n",
       "84919                        Robot de piscine √©lectrique   \n",
       "84920  Hsm Destructeur Securio C16 Coupe Crois¬øE: 4 X...   \n",
       "\n",
       "                                             description   productid  \\\n",
       "84916                                               <NA>   516376098   \n",
       "84917                                               <NA>   133389013   \n",
       "84918                                               <NA>  4128438366   \n",
       "84919  <p>Ce robot de piscine d&#39;un design innovan...  3929899732   \n",
       "84920                                               <NA>   152993898   \n",
       "\n",
       "          imageid                               image_name  \\\n",
       "84916  1019294171   image_1019294171_product_516376098.jpg   \n",
       "84917  1274228667   image_1274228667_product_133389013.jpg   \n",
       "84918  1295960357  image_1295960357_product_4128438366.jpg   \n",
       "84919  1265224052  image_1265224052_product_3929899732.jpg   \n",
       "84920   940543690    image_940543690_product_152993898.jpg   \n",
       "\n",
       "                                                    text  \n",
       "84916  folkmanis puppets marionnette theatre mini turtle  \n",
       "84917   porte flamme gaxix flamebringer twilight dragons  \n",
       "84918                        pompe filtration speck badu  \n",
       "84919  robot piscine electrique robot design innovant...  \n",
       "84920               hsm destructeur securio coupe croise  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(config)  # Reload config to ensure any updates are applied\n",
    "\n",
    "# Define paths for datasets\n",
    "train_pickle_path = Path(config.XTRAIN_FINAL_ENCODED_PATH)\n",
    "# train_pickle_path = Path(config.XTRAIN_FINAL_PATH)\n",
    "test_pickle_path = Path(config.XTEST_SUB_FINAL_PATH)\n",
    "y_train_pickle_path = Path(config.YTRAIN_FINAL_PATH)\n",
    "y_train_encoded_pickle_path = Path(config.YTRAIN_ENCODED_PATH)\n",
    "\n",
    "# Function to get relative paths from project root\n",
    "def get_relative_path(absolute_path: Path):\n",
    "    \"\"\"Returns the relative path from the project root.\"\"\"\n",
    "    return str(absolute_path.relative_to(config.BASE_DIR))\n",
    "\n",
    "# Function to load a Pickle file safely\n",
    "def load_pickle(file_path: Path, dataset_name: str):\n",
    "    \"\"\"Loads a pickle file with error handling and basic visualization.\"\"\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"Error: `{dataset_name}` file not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        data = pd.read_pickle(file_path)\n",
    "        print(f\"Successfully loaded `{dataset_name}` | Shape: {data.shape}\")\n",
    "        \n",
    "        # Display first rows if dataset is not empty\n",
    "        if not data.empty:\n",
    "            display(data.head())\n",
    "\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading `{dataset_name}`: {e}\")\n",
    "        return None\n",
    "\n",
    "# List of required files with their names\n",
    "required_files = {\n",
    "    \"Training Dataset\": train_pickle_path,\n",
    "    \"Testing Dataset\": test_pickle_path,\n",
    "    \"Encoded Training Labels\": y_train_encoded_pickle_path,\n",
    "    \"Final Training Labels\": y_train_pickle_path\n",
    "}\n",
    "\n",
    "# Check if files exist before loading\n",
    "for name, path in required_files.items():\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Error: `{name}` file not found at {get_relative_path(path)}\")\n",
    "\n",
    "# Load datasets\n",
    "X_train_full = load_pickle(train_pickle_path, \"X_train_final_encoded.pkl\")\n",
    "y_train_encoded = load_pickle(y_train_encoded_pickle_path, \"y_train_encoded.pkl\")\n",
    "X_test_Submission = load_pickle(test_pickle_path, \"X_test_Submission_final.pkl\") # Submission dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f13f0",
   "metadata": {},
   "source": [
    "## 3. Apply Tokenization and Sequencing  \n",
    "\n",
    "\n",
    "******************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4cc97",
   "metadata": {},
   "source": [
    "### 3.1 Splitting Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14fe3a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Shape : (67932, 9)\n",
      " Testing Shape : (16984, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# D√©finir les param√®tres\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 1234\n",
    "\n",
    "# Split en conservant toutes les colonnes\n",
    "X_train_full, X_test_full = train_test_split(X_train_full, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "# V√©rification\n",
    "print(f\" Training Shape : {X_train_full.shape}\")\n",
    "print(f\" Testing Shape : {X_test_full.shape}\")\n",
    "\n",
    "#  Sauvegarde des splits\n",
    "X_train_full.to_pickle(Path(config.PROCESSED_DIR) / \"X_train_split.pkl\")\n",
    "X_test_full.to_pickle(Path(config.PROCESSED_DIR) / \"X_test_split.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec87bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = X_train_full[\"text\"]\n",
    "X_test_text = X_test_full[\"text\"]\n",
    "\n",
    "X_test_Submission_text = X_test_Submission[\"text\"]\n",
    "\n",
    "y_train_text = X_train_full[\"prdtypecode_encoded\"]\n",
    "y_test_text = X_test_full[\"prdtypecode_encoded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7100a3d",
   "metadata": {},
   "source": [
    "### 3.4 Define and Train the Tokenizer - Converting Text to Numerical Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd041dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 20000\n",
      "Wall time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the tokenizer with a max vocabulary size\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "\n",
    "\n",
    "# Fit the tokenizer on the training text data\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "# Define word-index mappings\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = tokenizer.index_word\n",
    "vocab_size = tokenizer.num_words\n",
    "\n",
    "# Print vocabulary size\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc4f40",
   "metadata": {},
   "source": [
    "### 3.3 Convert Text Data to Tokenized Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3b51dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokenized sentence (before padding): [43, 39, 179, 1967, 36, 3316, 4045, 802, 400, 180, 179, 1967, 36, 828, 43, 17, 31, 1595, 938, 1322, 2805, 352, 39, 3252, 932, 614, 1036, 2058, 15, 2738, 1448, 236, 1, 3137, 757, 610, 682, 61, 862, 14877, 113, 2751, 298, 830, 1032, 834, 932, 3379, 515, 1389, 716, 1392, 253, 5624, 680, 2519, 1207, 284, 573, 10709, 2388, 149, 239, 2253, 4404, 237, 21, 1061, 2423, 122, 250, 1151, 179, 144, 249, 510, 4981, 35, 617, 8160, 5970, 810, 2718, 68, 968, 46, 132, 160, 5, 828, 5177, 280, 12424, 3517, 1066, 2859, 10193, 6871, 459, 933, 1404, 447, 66, 9732, 847, 2566, 879, 2790, 46, 1304, 11659, 6442, 8, 4508, 1795, 126, 113, 1952, 1274, 2058, 193, 2, 3232, 5514, 22, 583, 84, 78, 153, 381, 238, 84, 48, 467, 3106]\n"
     ]
    }
   ],
   "source": [
    "# Convert text data to sequences\n",
    "X_train_text = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_text = tokenizer.texts_to_sequences(X_test_text)\n",
    "X_test_Submission_text  = tokenizer.texts_to_sequences(X_test_Submission_text)  # Submission dataset\n",
    "# Print example sequences\n",
    "print(\"Example tokenized sentence (before padding):\", X_train_text[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef0655c",
   "metadata": {},
   "source": [
    "### 3.4 Apply Padding to Standardize Input Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1ee6835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (67932, 500)\n",
      "Testing data shape: (16984, 500)\n",
      "Submission data shape: (13812, 500)\n"
     ]
    }
   ],
   "source": [
    "# Define the maximum sequence length\n",
    "maxlen = 500  \n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "X_train_text = tf.keras.preprocessing.sequence.pad_sequences(X_train_text, maxlen=maxlen, padding='post')\n",
    "X_test_text = tf.keras.preprocessing.sequence.pad_sequences(X_test_text, maxlen=maxlen, padding='post')\n",
    "\n",
    "X_test_Submission_text = tf.keras.preprocessing.sequence.pad_sequences(X_test_Submission_text, maxlen=maxlen, padding='post')  # Submission dataset\n",
    "\n",
    "# Print shape to confirm\n",
    "print(f\"Training data shape: {X_train_text.shape}\")\n",
    "print(f\"Testing data shape: {X_test_text.shape}\")\n",
    "print(f\"Submission data shape: {X_test_Submission_text.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5b2f4",
   "metadata": {},
   "source": [
    "### 3.5 Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "27ed4245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed tokenizer data saved at: D:\\Data_Science\\Append_Data_Engineer_AWS_MLOPS\\Data_Scientist_Rakuten_Project-main\\data\\processed\\text\\tokenizer.pkl\n",
      "\n",
      "Processed training data saved at: D:\\Data_Science\\Append_Data_Engineer_AWS_MLOPS\\Data_Scientist_Rakuten_Project-main\\data\\processed\\text\\X_train_TokenizationSequencing.pkl\n",
      "\n",
      "Processed test data saved at: D:\\Data_Science\\Append_Data_Engineer_AWS_MLOPS\\Data_Scientist_Rakuten_Project-main\\data\\processed\\text\\X_test_TokenizationSequencing.pkl\n",
      "\n",
      "y_train data saved at: D:\\Data_Science\\Append_Data_Engineer_AWS_MLOPS\\Data_Scientist_Rakuten_Project-main\\data\\processed\\text\\y_train_split.pkl\n",
      "\n",
      "x_train test data saved at: D:\\Data_Science\\Append_Data_Engineer_AWS_MLOPS\\Data_Scientist_Rakuten_Project-main\\data\\processed\\text\\y_test_split.pkl\n",
      "\n",
      "Processed submission data saved at: D:\\Data_Science\\Append_Data_Engineer_AWS_MLOPS\\Data_Scientist_Rakuten_Project-main\\data\\processed\\text\\X_submission_TokenizationSequencing.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define save paths\n",
    "TOKENIZER_PATH = Path(config.PROCESSED_TEXT_DIR) / \"tokenizer.pkl\"\n",
    "X_TRAIN_SPLIT_TOKENIZED_PATH = Path(config.PROCESSED_TEXT_DIR) / \"X_train_TokenizationSequencing.pkl\"\n",
    "X_TEST_SPLIT_TOKENIZED_PATH = Path(config.PROCESSED_TEXT_DIR) / \"X_test_TokenizationSequencing.pkl\"\n",
    "Y_TRAIN_SPLIT_PATH  = Path(config.PROCESSED_TEXT_DIR) / \"y_train_split.pkl\"\n",
    "Y_TEST_SPLIT_PATH = Path(config.PROCESSED_TEXT_DIR) / \"y_test_split.pkl\"\n",
    "X_SUBMISSION_TOKENIZED_PATH = Path(config.PROCESSED_TEXT_DIR) / \"X_submission_TokenizationSequencing.pkl\"\n",
    "\n",
    "\n",
    "# Save tokenized & padded datasets using Pickle\n",
    "with open(TOKENIZER_PATH, \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "    \n",
    "    \n",
    "with open(X_TRAIN_SPLIT_TOKENIZED_PATH, \"wb\") as f:\n",
    "    pickle.dump(X_train, f)\n",
    "\n",
    "with open(X_TEST_SPLIT_TOKENIZED_PATH, \"wb\") as f:\n",
    "    pickle.dump(X_test, f)\n",
    "    \n",
    "with open(Y_TRAIN_SPLIT_PATH, \"wb\") as f:\n",
    "    pickle.dump(y_train, f)\n",
    "    \n",
    "with open(Y_TEST_SPLIT_PATH, \"wb\") as f:\n",
    "    pickle.dump(y_test, f)\n",
    "\n",
    "with open(X_SUBMISSION_TOKENIZED_PATH, \"wb\") as f:\n",
    "    pickle.dump(X_submission, f)\n",
    "\n",
    "# Check if files were saved successfully before printing\n",
    "\n",
    "if TOKENIZER_PATH.exists():\n",
    "    print(f\"\\nProcessed tokenizer data saved at: {TOKENIZER_PATH}\")\n",
    "else:\n",
    "    print(\"Error: Training data file was not saved!\")\n",
    "    \n",
    "if X_TRAIN_TOKENIZED_PATH.exists():\n",
    "    print(f\"\\nProcessed training data saved at: {X_TRAIN_SPLIT_TOKENIZED_PATH}\")\n",
    "else:\n",
    "    print(\"Error: Training data file was not saved!\")\n",
    "\n",
    "if X_TEST_TOKENIZED_PATH.exists():\n",
    "    print(f\"\\nProcessed test data saved at: {X_TEST_SPLIT_TOKENIZED_PATH}\")\n",
    "else:\n",
    "    print(\"Error: Test data file was not saved!\")\n",
    "    \n",
    "if Y_TRAIN_SPLIT_PATH.exists():\n",
    "    print(f\"\\ny_train data saved at: {Y_TRAIN_SPLIT_PATH}\")\n",
    "else:\n",
    "    print(\"Error: y_train data file was not saved!\")\n",
    "\n",
    "if Y_TEST_SPLIT_PATH.exists():\n",
    "    print(f\"\\nx_train test data saved at: {Y_TEST_SPLIT_PATH}\")\n",
    "else:\n",
    "    print(\"Error: x_train data file was not saved!\")\n",
    "\n",
    "if X_SUBMISSION_TOKENIZED_PATH.exists():\n",
    "    print(f\"\\nProcessed submission data saved at: {X_SUBMISSION_TOKENIZED_PATH}\")\n",
    "else:\n",
    "    print(\"Error: Submission data file was not saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7c52e1",
   "metadata": {},
   "source": [
    "## 4. üîÑNext Steps  \n",
    "\n",
    "In this notebook, we have preprocessed the text data by tokenizing and padding the sequences, preparing them for deep learning models. The following processed data has been saved for future use:\n",
    "- The **tokenizer** object, which contains the word-to-index mapping for tokenization.\n",
    "- The **tokenized and padded sequences** for the training, test, and submission datasets.\n",
    "\n",
    "---\n",
    "We have now completed the exploration and preprocessing of textual data, from the initial dataset analysis to text cleaning and visualization using Word Clouds, as well as text feature extraction for machine learning models and text tokenization and sequence preparation for deep learning models.\n",
    "\n",
    "The following notebooks were executed as part of this process:\n",
    "\n",
    " **1_Project_and_Data_Overview.ipynb** ‚Üí Initial project and data exploration  \n",
    " **2_CSV_Exploration_and_Visualization.ipynb** ‚Üí CSV data exploration and visualization  \n",
    " **3_Image_Exploration_and_Visualization.ipynb** ‚Üí Image dataset analysis  \n",
    " **4_Text_Cleaning.ipynb** ‚Üí Text preprocessing and cleaning  \n",
    " **5_Text_WordClouds_for_Product_Categories.ipynb** ‚Üí Visualizing text data through word clouds  \n",
    " **6_ML_Text_Vectorization_TF-IDF.ipynb** ‚Üí Text feature extraction for machine learning models  \n",
    " **7_DL_Text_Tokenization_and_Sequencing.ipynb** ‚Üí Text tokenization and sequence preparation for deep learning models\n",
    "\n",
    "---\n",
    "‚û°Ô∏è \n",
    "We will now move on to the modeling phase, starting with classic machine learning models for text classification, such as **Logistic Regression**, **Random Forest**, etc., to establish a baseline performance. We will then explore deep learning architectures like **RNNs**, **LSTMs**, or **GRUs** for more advanced modeling. After optimizing both approaches for text, we will shift our focus to image data using **CNNs** (Convolutional Neural Networks). Once both text and image models are optimized, we will explore a **multimodal** (text + image) approach to enhance classification performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b3206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
