{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a3a4f1",
   "metadata": {},
   "source": [
    "# Text Cleaning  \n",
    "\n",
    "## 📌 Notebook Objective  \n",
    "\n",
    "In this notebook, we focus on **cleaning and preprocessing text data** to enhance the quality of textual features for modeling.  \n",
    "\n",
    "Text cleaning is a crucial step in **Natural Language Processing (NLP)**, ensuring that the input data is **consistent, structured, and noise-free**. This will **improve model performance** for text-based classification.  \n",
    "\n",
    "###  **Key Steps**  \n",
    "\n",
    "✔ **Loading structured datasets** → Import cleaned product metadata (`X_train_img.pkl` & `X_test_img.pkl`).  \n",
    "✔ **Creating a unified text column** → Merging designation (title) and description.  \n",
    "✔ **Cleaning text** → Lowercasing, removing accents, stripping HTML tags, and filtering special characters.  \n",
    "✔ **Removing stopwords and shor words** → Eliminating common, unimportant words in French, English, and German, as well as words with fewer than 3 characters.                \n",
    "✔ **Finalizing and saving** → Ensuring cleaned text is ready for further tasks and storing it as (`df_train_cleaned.pkl` & `df_test_cleaned.pkl`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5aab18",
   "metadata": {},
   "source": [
    "## 1. Load Pickle Files (X_train_img.pkl & X_test_img.pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21106d8",
   "metadata": {},
   "source": [
    "###  Import Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048fd7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "import html\n",
    "import importlib\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b054b",
   "metadata": {},
   "source": [
    "#### Setting Up Project Paths and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1529d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current notebook directory\n",
    "CURRENT_DIR = Path(os.getcwd()).resolve()\n",
    "\n",
    "# Automatically find the project root (go up 1 level)\n",
    "PROJECT_ROOT = CURRENT_DIR.parents[1]\n",
    "\n",
    "# Add project root to sys.path\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Function to get relative paths from project root\n",
    "def get_relative_path(absolute_path):\n",
    "    return str(Path(absolute_path).relative_to(PROJECT_ROOT))\n",
    "\n",
    "# Print project root directory\n",
    "print(f\"Project Root Directory: {PROJECT_ROOT.name}\")  # Display only the root folder name\n",
    "\n",
    "import config  # Now Python can find config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72229c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload config to ensure any updates are applied\n",
    "importlib.reload(config)\n",
    "\n",
    "# Define directory for interim text-related pickle files\n",
    "INTERIM_DIR = Path(config.INTERIM_DIR)\n",
    "\n",
    "INTERIM_DIR.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "# Define file paths for storing text-related datasets\n",
    "TRAIN_PICKLE_PATH = INTERIM_DIR / \"X_train_full_img.pkl\"\n",
    "TEST_PICKLE_PATH = INTERIM_DIR / \"X_test_img_sub.pkl\"\n",
    "\n",
    "# Adjust the display width for columns\n",
    "pd.set_option('display.max_colwidth', 500)  #You can adjust it\n",
    "\n",
    "# Function to load a Pickle file safely\n",
    "def load_pickle(file_path, dataset_name):\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            data = pd.read_pickle(file_path)\n",
    "            print(f\"Successfully loaded `{dataset_name}` | Shape: {data.shape}\\n\")\n",
    "            display(data.head())  # Display first few rows\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading `{dataset_name}`: {e}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    return None\n",
    "\n",
    "# Load both datasets\n",
    "X_train = load_pickle(TRAIN_PICKLE_PATH, \"X_train_full_img.pkl\")\n",
    "X_test_sub = load_pickle(TEST_PICKLE_PATH, \"X_test_img_sub.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3304bf",
   "metadata": {},
   "source": [
    "## **2. Generating a Unified Text Column**  \n",
    "\n",
    "In the previous analysis of **CSV Exploration and Visualization**, we observed:  \n",
    "\n",
    "- **35.1%** of `description` is missing in `df_xtrain` and **35.4%** in `df_xtest`.  \n",
    "- `designation` (product title) is always present.  \n",
    "- The `description` field, when available, provides additional context about the product.  \n",
    "\n",
    " **Action Plan:**  \n",
    "To handle missing values and **enrich product information**, we will:  \n",
    "✔**combine `designation` and `description`** into a single **text column**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert designation and description to string type explicitly\n",
    "X_train[\"designation\"] = X_train[\"designation\"].astype(\"string\")\n",
    "X_test_sub[\"designation\"] = X_test_sub[\"designation\"].astype(\"string\")\n",
    "\n",
    "X_train[\"description\"] = X_train[\"description\"].astype(\"string\")\n",
    "X_test_sub[\"description\"] = X_test_sub[\"description\"].astype(\"string\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to remove duplicate words while preserving order\n",
    "def remove_duplicate_words(text):\n",
    "    words = text.split()\n",
    "    unique_words = list(dict.fromkeys(words))  # Removes duplicates while keeping order\n",
    "    return \" \".join(unique_words)\n",
    "\n",
    "# Function to create a cleaned text column\n",
    "def create_clean_text(designation, description):\n",
    "    \"\"\"\n",
    "    Combines 'designation' and 'description' into a single text column.\n",
    "    - Handles missing descriptions\n",
    "    - Merges text fields and removes duplicate words\n",
    "    \"\"\"\n",
    "    if pd.isna(description) or description.strip() == \"\":\n",
    "        text = designation\n",
    "    else:\n",
    "        text = f\"{designation} {description}\"\n",
    "\n",
    "    return remove_duplicate_words(text)  # Apply duplicate removal\n",
    "\n",
    "# Apply functions to create the cleaned 'text' column\n",
    "X_train[\"text\"] = X_train.apply(lambda row: create_clean_text(row[\"designation\"], row[\"description\"]), axis=1)\n",
    "X_test_sub[\"text\"] = X_test_sub.apply(lambda row: create_clean_text(row[\"designation\"], row[\"description\"]), axis=1)\n",
    "\n",
    "# Display sample results\n",
    "print(\" Sample merged text column (Training Data):\")\n",
    "display(X_train[[\"designation\", \"description\", \"text\"]].head())\n",
    "\n",
    "print(\"\\n Sample merged text column (Submission Data):\")\n",
    "display(X_test_sub[[\"designation\", \"description\", \"text\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63877dc7",
   "metadata": {},
   "source": [
    "## 3. Cleaning text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4a604",
   "metadata": {},
   "source": [
    "### 3.1 Converting Text to Lowercase\n",
    "\n",
    "To ensure consistency in text processing, we convert all text to **lowercase**.  \n",
    "This helps in **avoiding mismatches** between words like `\"Laptop\"` and `\"laptop\"`, which should be treated as the same word.  \n",
    "We also remove extra spaces at the beginning and end of the text to make it cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert text to lowercase\n",
    "def lower_case(text):\n",
    "    return text.lower().strip()\n",
    "\n",
    "# Apply transformation\n",
    "X_train[\"text\"] = X_train[\"text\"].apply(lower_case)\n",
    "X_test_sub[\"text\"] = X_test_sub[\"text\"].apply(lower_case)\n",
    "\n",
    "# Display sample results\n",
    "print(\" Sample cleaned text (Training Data):\")\n",
    "display(X_train[[\"text\"]].head())\n",
    "\n",
    "print(\"\\n Sample cleaned text (Testing Data):\")\n",
    "display(X_test_sub[[\"text\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdbe05f",
   "metadata": {},
   "source": [
    "### 3.2  Decode HTML entities\n",
    "This step will decode any HTML entities in the text (e.g., `&eacute;` becomes `é`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def decode_html_entities(text):\n",
    "    \"\"\"\n",
    "    Decodes HTML entities in the text (e.g., &eacute; to é).\n",
    "    \"\"\"\n",
    "    return html.unescape(text)\n",
    "\n",
    "# Apply the function to the text columns\n",
    "X_train['text'] = X_train['text'].apply(decode_html_entities)\n",
    "X_test_sub['text'] = X_test_sub['text'].apply(decode_html_entities)\n",
    "\n",
    "# Display a sample to verify\n",
    "print(\"Sample text after decoding HTML entities (Training Data):\")\n",
    "display(X_train[['text']].head())\n",
    "\n",
    "print(\"\\nSample text after decoding HTML entities (Testing Data):\")\n",
    "display(X_test_sub[['text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed20139",
   "metadata": {},
   "source": [
    "### 3.3 Remove HTML Tags\n",
    "This step removes any remaining HTML tags such as `<p>`, `<b>`, `<br>`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a059fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"\n",
    "    Removes HTML tags from the given text using regex substitution.\n",
    "    This ensures that only meaningful product information remains.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"<[^<]+?>\", \"\", text)\n",
    "\n",
    "# Apply the function to the text columns\n",
    "X_train['text'] = X_train['text'].apply(remove_html_tags)\n",
    "X_test_sub['text'] = X_test_sub['text'].apply(remove_html_tags)\n",
    "\n",
    "# Display a sample to verify\n",
    "print(\"Sample text after removing HTML tags (Training Data):\")\n",
    "display(X_train[['text']].head())\n",
    "\n",
    "print(\"\\nSample text after removing HTML tags (Testing Data):\")\n",
    "display(X_test_sub[['text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a446335",
   "metadata": {},
   "source": [
    "### 3.4 Removing Accents from Text\n",
    "This step will remove accents from characters in the text (e.g., `é` becomes `e`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove accented characters from text\n",
    "def remove_accent(text):\n",
    "    \"\"\"\n",
    "    Replaces accented characters with their non-accented counterparts.\n",
    "    This ensures consistency in text processing and improves model robustness.\n",
    "    \"\"\"\n",
    "    text = text.replace('á', 'a').replace('â', 'a')\n",
    "    text = text.replace('é', 'e').replace('è', 'e').replace('ê', 'e').replace('ë', 'e')\n",
    "    text = text.replace('î', 'i').replace('ï', 'i')\n",
    "    text = text.replace('ö', 'o').replace('ô', 'o').replace('ò', 'o').replace('ó', 'o')\n",
    "    text = text.replace('ù', 'u').replace('û', 'u').replace('ü', 'u')\n",
    "    text = text.replace('ç', 'c')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply transformation\n",
    "X_train[\"text\"] = X_train[\"text\"].apply(remove_accent)\n",
    "X_test_sub[\"text\"] = X_test_sub[\"text\"].apply(remove_accent)\n",
    "\n",
    "# Display sample results\n",
    "print(\" Sample text after removing accents (Training Data):\")\n",
    "display(X_train[[\"text\"]].head())\n",
    "\n",
    "print(\"\\n Sample text after removing accents (Testing Data):\")\n",
    "display(X_test_sub[[\"text\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abbcb98",
   "metadata": {},
   "source": [
    "### 3.5  Normalize Text by Replacing Special Characters\n",
    "This step replaces certain special characters and typographic symbols with their standard equivalents. This ensures consistency and helps in preparing the text for further processing.\n",
    "\n",
    "We will replace:\n",
    "- **Smart quotes** with regular quotes\n",
    "- **En dashes** and **em dashes** with a hyphen\n",
    "- **Ellipses** with three dots\n",
    "- Remove unwanted characters (e.g., `¿`)\n",
    "\n",
    "This will further clean up the text and avoid unwanted characters affecting model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Replace special characters with their standard equivalents.\n",
    "    This step includes replacing smart quotes, dashes, ellipses, and removing unwanted characters.\n",
    "    \"\"\"\n",
    "    replacements = {\n",
    "        \"’\": \"'\",    # Smart quote → standard quote\n",
    "        \"‘\": \"'\",    # Smart quote → standard quote\n",
    "        \"“\": '\"',    # Smart quote → standard quote\n",
    "        \"”\": '\"',    # Smart quote → standard quote\n",
    "        \"–\": \"-\",    # En dash → hyphen\n",
    "        \"—\": \"-\",    # Em dash → hyphen\n",
    "        \"…\": \"...\",  # Ellipsis → three dots\n",
    "        \"¿\": \"\",     # Remove unwanted character\n",
    "    }\n",
    "    for key, value in replacements.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text\n",
    "\n",
    "# Apply the function to the text columns\n",
    "X_train[\"text\"] = X_train[\"text\"].apply(normalize_text)\n",
    "X_test_sub[\"text\"] = X_test_sub[\"text\"].apply(normalize_text)\n",
    "\n",
    "# Display sample results\n",
    "print(\"Sample text after normalizing special characters (Training Data):\")\n",
    "display(X_train[[\"text\"]].head())\n",
    "\n",
    "print(\"\\nSample text after normalizing special characters (Testing Data):\")\n",
    "display(X_test_sub[[\"text\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a555671",
   "metadata": {},
   "source": [
    "### 3.6. Keeping Essential Characters Only\n",
    "In this step, we will remove any non-alphabetic characters (e.g., numbers, punctuation) and keep only letters. This helps in focusing on meaningful words for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keeping_essentiel(text):\n",
    "    \"\"\"\n",
    "    Removes all non-alphabetic characters and keeps only letters.\n",
    "    This ensures that the text contains only relevant words and characters.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^a-zA-Z]+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "# Apply the function to the text columns\n",
    "X_train['text'] = X_train['text'].apply(lambda text: keeping_essentiel(text))\n",
    "X_test_sub['text'] = X_test_sub['text'].apply(lambda text: keeping_essentiel(text))\n",
    "\n",
    "# Display a sample to verify\n",
    "print(\"Sample text after keeping only essential characters (Training Data):\")\n",
    "display(X_train[['text']].head())\n",
    "\n",
    "print(\"\\nSample text after keeping only essential characters (Testing Data):\")\n",
    "display(X_test_sub[['text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715809a7",
   "metadata": {},
   "source": [
    "### 3.7 Removing Stopwords and Short Words\n",
    "\n",
    "In this step, we will remove **stopwords** (commonly used words like \"the\", \"and\", \"is\", etc.) and **short words** (words with fewer than 3 characters) from the text. These words do not add significant meaning to the text and can introduce noise, making it harder for the model to learn useful patterns.\n",
    "\n",
    "The function `remove_stopwords_and_short_words` will:\n",
    "- Eliminate stopwords from the text, including those from French, English, and German, as well as custom stopwords specific to our dataset.\n",
    "- Remove any words that are shorter than 3 characters.\n",
    "\n",
    "This helps ensure that only meaningful words remain in the dataset, improving model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the stop words variable\n",
    "stop_words = (stopwords.words('french') \n",
    "              + stopwords.words('english') \n",
    "              + stopwords.words('german') \n",
    "              + ['plus', 'peut', 'tout', 'etre', 'sans', 'dont', 'aussi', 'comme', 'meme', 'bien', \n",
    "                 'leurs', 'elles', 'cette', 'celui', 'ainsi', 'encore', 'alors', 'toujours', 'toute', \n",
    "                 'deux', 'nouveau', 'peu', 'car', 'autre', 'jusqu', 'quand', 'ici', 'ceux', 'enfin', \n",
    "                 'jamais', 'autant', 'tant', 'avoir', 'moin', 'celle', 'tous', 'contre', 'pourtant', \n",
    "                 'quelque', 'toutes', 'surtout', 'cet', 'comment', 'rien', 'avant', 'doit', 'autre', \n",
    "                 'depuis', 'moins', 'tre', 'souvent', 'etait', 'pouvoir', 'apre', 'non', 'ver', 'quel', \n",
    "                 'pourquoi', 'certain', 'fait', 'faire', 'sou', 'donc', 'trop', 'quelques', 'parfois', \n",
    "                 'tres', 'donc', 'dire', 'eacute', 'egrave', 'rsquo', 'agrave', 'ecirc', 'nbsp', 'acirc', \n",
    "                 'apres', 'autres', 'ocirc', 'entre', 'sous', 'quelle'])\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords_and_short_words(list):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a list of words and filters out words that are too short (less than 3 characters).\n",
    "    Only words that are not in the stopwords list and have more than 2 characters will be kept.\n",
    "    \"\"\"\n",
    "    filtered_words  = []\n",
    "    for mot in list:\n",
    "        if (mot not in stop_words and len(mot) > 2):\n",
    "            filtered_words .append(mot)\n",
    "    return filtered_words \n",
    "\n",
    "# Split the text into individual words (tokens)\n",
    "X_train['text'] = X_train['text'].str.split()\n",
    "X_test_sub['text'] = X_test_sub['text'].str.split()\n",
    "\n",
    "# Apply stopwords removal and filter short words from both datasets\n",
    "X_train['text'] = X_train['text'].apply(lambda x: remove_stopwords_and_short_words(x))\n",
    "X_test_sub['text'] = X_test_sub['text'].apply(lambda x: remove_stopwords_and_short_words(x))\n",
    "\n",
    "# Join the words back into a string\n",
    "X_train['text'] = X_train['text'].apply(lambda x: \" \".join(x))\n",
    "X_test_sub['text'] = X_test_sub['text'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Display a sample to verify\n",
    "print(\"Sample text after removing stopwords and short words (Training Data):\")\n",
    "display(X_train[['text']].head())\n",
    "\n",
    "print(\"\\nSample text after removing stopwords and short words (Testing Data):\")\n",
    "display(X_test_sub[['text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df1a456",
   "metadata": {},
   "source": [
    "### 3.8 Removing Punctuation\n",
    "\n",
    "Punctuation marks such as periods, commas, question marks, exclamation points, parentheses, and quotation marks do not generally contribute meaningful information for most text classification tasks. Therefore, we will remove them to clean up the text and ensure that only the relevant words are processed.\n",
    "\n",
    "In this step:\n",
    "- We use the `string.punctuation` library to identify and remove all punctuation marks from the text.\n",
    "- This ensures the model focuses on the core content without being distracted by unnecessary symbols.\n",
    "\n",
    "The function `remove_punctuation` will be applied to both the training and testing datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce777db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Remove punctuation using Python string.punctuation\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Apply this function to both datasets\n",
    "X_train['text'] = X_train['text'].apply(remove_punctuation)\n",
    "X_test_sub['text'] = X_test_sub['text'].apply(remove_punctuation)\n",
    "\n",
    "\n",
    "# Display a sample to verify\n",
    "print(\"Sample text after removing punctuation(Training Data):\")\n",
    "display(X_train[['text']].head())\n",
    "\n",
    "print(\"\\nSample text after removing punctuation (Testing Data):\")\n",
    "display(X_test_sub[['text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b949c902",
   "metadata": {},
   "source": [
    "# 4. Saving Updated Datasets for Future Use\n",
    "\n",
    "To avoid reloading and recomputing the datasets in every notebook, we save the cleaned datasets as Pickle files.\n",
    "\n",
    "In this notebook, we have performed the following:\n",
    "- Combined the `designation` (title) and `description` into a single column called `text`.\n",
    "- Cleaned the text data by converting it to lowercase, removing accents, stripping HTML tags, filtering special characters, and eliminating stopwords and short words.\n",
    "\n",
    "We will store the following updated datasets for future use:\n",
    "- **`X_train_cleaned.pkl`** → The training dataset with cleaned text.\n",
    "- **`X_test_cleaned.pkl`** → The test dataset with cleaned text.\n",
    "\n",
    "These files will be used in future steps, including feature engineering and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8bdf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the directory and file names\n",
    "pickle_dir = \"../data/interim/\"\n",
    "os.makedirs(pickle_dir, exist_ok=True)\n",
    "\n",
    "# Define file paths\n",
    "train_pickle_path = os.path.join(pickle_dir, \"X_train_cleaned.pkl\")\n",
    "test_pickle_path = os.path.join(pickle_dir, \"X_test_sub_cleaned.pkl\")\n",
    "\n",
    "try:\n",
    "    # Save updated training dataset\n",
    "    X_train.to_pickle(train_pickle_path)\n",
    "    print(f\"Training dataset saved: {train_pickle_path}\")\n",
    "\n",
    "    # Save updated test dataset\n",
    "    X_test_sub.to_pickle(test_pickle_path)\n",
    "    print(f\"Test dataset saved: {test_pickle_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saving datasets: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a93a7bb",
   "metadata": {},
   "source": [
    "## 5. 🔄 Next Steps  \n",
    "\n",
    "Now that we have cleaned the text data, the next step is to enhance our understanding of the dataset through **visualizations** and further analysis.\n",
    "\n",
    "We will utilize the cleaned text data to:\n",
    "- Create **WordClouds** to visually represent the most frequent words in different classes.\n",
    "- Identify key labels for each class, such as:\n",
    "  - 50: 'video games accessories'\n",
    "  - 2705: 'books'\n",
    "  - ...and more.\n",
    "\n",
    "The **WordClouds** will help us to visually explore and understand the dominant themes for each product category.\n",
    "\n",
    "---\n",
    "➡️ **Proceed to `5_Text_WordClouds_for_Product_Categories.ipynb`**  \n",
    "This notebook will focus on generating WordClouds for each product category and analyzing the textual content of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ecf82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
